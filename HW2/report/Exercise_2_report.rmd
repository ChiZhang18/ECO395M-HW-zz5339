---
title: "Exercise 1"
author: "Chong Wang, Tianping Wu, Zhenning Zhao"
date: "2019/3/3"
output:
  md_document:
    variant: markdown_github
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(tidyverse)
library(mosaic)
library(class)
library(FNN)
```

# Exercise 2
By Chong Wang, Tianping Wu, Zhenning Zhao

# Exercise 2.1

```{r setup_2.1, include=FALSE}
data(SaratogaHouses)
summary(SaratogaHouses)

# define the functions
rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}
KNN_avgrmse = function(data_X, data_y, K = 2, Ntimes = 50){
  n = nrow(data_X)
  rmse_vals_K = do(Ntimes)*{
    # re-split into train and test cases
    n_train = round(0.8*n)  # round to nearest integer
    n_test = n - n_train
    train_cases = sample.int(n, n_train, replace=FALSE)
    test_cases = setdiff(1:n, train_cases)
    X_train = data_X[train_cases,]
    X_test = data_X[test_cases,]
    y_train = data_y[train_cases,]
    y_test = data_y[test_cases,]
    
    # scaling
    scale_factors = apply(X_train, 2, sd)
    X_train_sc = scale(X_train, scale=scale_factors)
    # scale the test set features using the same scale factors
    X_test_sc = scale(X_test, scale=scale_factors)
    
    
    # Fit the KNN model (notice the odd values of K)
    knn_K = FNN::knn.reg(train=X_train_sc, test= X_test_sc, y = y_train, k=K)
    ypred_knn = knn_K$pred
    rmse(y_test, ypred_knn)
  }
  colMeans(rmse_vals_K)
}

# Setup for KNN: change from string to numeric
temp = model.matrix( ~ heating-1,SaratogaHouses)
colnames(temp)<- c("heatinghotair", "heatingwatersteam", "heatingelectric")
SaratogaHouses = cbind(SaratogaHouses,temp)
temp = model.matrix( ~ fuel-1,SaratogaHouses)
SaratogaHouses = cbind(SaratogaHouses,temp)
temp = model.matrix( ~ centralAir-1,SaratogaHouses)
SaratogaHouses = cbind(SaratogaHouses,temp)

data = SaratogaHouses

n = nrow(SaratogaHouses)
Ntimes = 100
# performance check
rmse_vals = do(Ntimes)*{
  # re-split into train and test cases
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  DF_train = data[train_cases,]
  DF_test = data[test_cases,]
  
  # fit to this training set
  lm_result1 = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=DF_train)
  lm_result2 = lm(price ~ lotSize + age + livingArea + pctCollege*fireplaces + bedrooms  + bathrooms + rooms + heating + centralAir, data=DF_train)
  lm_result3 = lm(price ~ landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=DF_train)
  lm_result4 = lm(price ~ landValue + livingArea + bedrooms + bathrooms + rooms + heating + fuel + centralAir, data=DF_train)
  # the best regression
  lm_result5 = lm(price ~ landValue + lotSize*(bedrooms + bathrooms) + livingArea*(fuel+ heating + centralAir) + pctCollege*(fireplaces+age) + rooms, data=DF_train)
  lm_result6 = lm(price ~ landValue + lotSize*bedrooms + livingArea*fuel + pctCollege*(fireplaces+age) + bathrooms + rooms + heating + centralAir, data=DF_train)
  lm_result7 = lm(price ~ landValue + lotSize*bedrooms + livingArea*fuel + pctCollege*(fireplaces+age) + centralAir, data=DF_train)
  
  # predict on this testing set
  yhat_test1 = predict(lm_result1, DF_test)
  yhat_test2 = predict(lm_result2, DF_test)
  yhat_test3 = predict(lm_result3, DF_test)
  yhat_test4 = predict(lm_result4, DF_test)
  yhat_test5 = predict(lm_result5, DF_test)
  yhat_test6 = predict(lm_result6, DF_test)
  yhat_test7 = predict(lm_result7, DF_test)
  
  c(rmse(unlist(DF_test$price), yhat_test1), rmse(unlist(DF_test$price), yhat_test2), rmse(unlist(DF_test$price), yhat_test3), rmse(unlist(DF_test$price), yhat_test4), rmse(unlist(DF_test$price), yhat_test5), rmse(unlist(DF_test$price), yhat_test6), rmse(unlist(DF_test$price), yhat_test7))
}
```

Long story short, we "hand-build" a model for price that outperforms the "medium" model that we considered in class, and the KNN model did not beat the “hand-build” model that we selected.

The models that we checked are listed below.

The performance of the models are mesured with average out-of-sample RMSE. We use the 80% of the data to do regressions and calculate RMSE for the rest 20%, and rerun the Monte Carlo training-testing split to calculate the average RMSE. The result of the models are listed below.

```{r table_2.1.1, echo=FALSE}
colnames(rmse_vals) <- c("model 1", "model 2", "model 3", "model 4", "model 5", "model 6", "model 7")
kable(colMeans(rmse_vals), col.names = c("AVG RMSE"), caption = "Table 2.1.1: The Average RMSE for Different Models",padding = 2, align = "c")
```

The best model that we solved is model 5. This model beats all the other models that we choose by having a smaller average RMSE of around 60000, while the average RMSE of the baseline model is around 66000. The regression result is:

```{r table_2.1.2, echo=FALSE}
table1 = summary(lm(price ~ landValue + 
                      lotSize*(bedrooms + bathrooms) + 
                      livingArea*(fuel+ heating + centralAir) + 
                      pctCollege*(fireplaces+age) + 
                      rooms, data=data))

kable(as.data.frame(table1["coefficients"]))
```

From the regression we can find many factors that could influence the house price.

First of all, the most important factor is the land value. When we include the land value into account, the RMSE dropped dramatically. It is clearly true that the higher the land value is, the higher the house price is.

Second, the room with more bedrooms have a lower price. This makes sense because more bedrooms means there are more people sharing the apartment, hence the utility of each person is dropping. On the other hand, the apartment with more bathrooms have a higher price. The apartment with larger living area have a higher price, and the price of the apartment drops as the age of the apartment grows. These conclusions matches our intuition.

Third, comparing with the apartments fueling with gas, the apartments fueling with oil have higher prices. comparing with the apartments having a central air conditioner, the apartments with separate air conditioner have higher price. What’s more, the apartments with a fireplace is more expensive, and the older the apartment is the lower the price is.

Forth, the interactions between the number of bedrooms or bathrooms and the lotsize are not significant, but the interactions between the living area and the fuel and whether the house has a central air conditioner is significant. To be more specific, the effect of the living area on the price of the room fueled with oil is lower than the room fueled with gas. And it is also true that the effect of the living area on the price of the room without a central air conditioner is lower than the room that has a central air conditioner.The interactions between whether the apartment is close to a college and the age of the house and the number of fireplaces are significant.

By using exactly same variables in the linear regression model it is very hard to beat the linear regression with the KNN model.

```{r graph2.1.1, echo=FALSE}
X = subset(SaratogaHouses, select=c(-price,-waterfront,-sewer,-newConstruction,-heating, -fuel, -centralAir, -centralAirNo, -fuelgas, -heatinghotair))
y = subset(SaratogaHouses, select=c(price))

KNN_result <- data.frame(K=c(), rsme=c())
k_grid = seq(3, 30, by=1)
for(v in k_grid){
avgrmse = KNN_avgrmse(data_X = X, data_y = y, K = v, Ntimes = 100)
KNN_result <- rbind(KNN_result,c(v,avgrmse))
}

colnames(KNN_result) <- c("K","AVG_RMSE")
ggplot(data = KNN_result, aes(x = K, y = AVG_RMSE)) + 
  geom_point(shape = "O") +
  geom_line(col = "red") +
  theme_bw()+
  labs(title = "AVG RMSE vs K (2<K<30)",
       x = "K",
       y = "AVG RMSE")+
  theme(plot.title = element_text(hjust = 0.5))
```

From the plot of average RMSE vs K we can tell the optimal K for the KNN model is about 10-15. The lowest average RMSE that we can get with KNN model is about 63000, which is still higher than the average RMSE that we conclude using a linear model with interactions.