---
title: "Exercise 1"
author: "Chong Wang, Tianping Wu, Zhenning Zhao"
date: "2019/3/3"
output:
  md_document:
    variant: markdown_github
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(tidyverse)
library(mosaic)
library(class)
library(FNN)
```

# Exercise 2
By Chong Wang, Tianping Wu, Zhenning Zhao

## Exercise 2.1

```{r setup_2.1, echo=FALSE, warning=FALSE}
data(SaratogaHouses)
# define the functions
rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}
KNN_avgrmse = function(data_X, data_y, K = 2, Ntimes = 50){
  n = nrow(data_X)
  rmse_vals_K = do(Ntimes)*{
    # re-split into train and test cases
    n_train = round(0.8*n)  # round to nearest integer
    n_test = n - n_train
    train_cases = sample.int(n, n_train, replace=FALSE)
    test_cases = setdiff(1:n, train_cases)
    X_train = data_X[train_cases,]
    X_test = data_X[test_cases,]
    y_train = data_y[train_cases,]
    y_test = data_y[test_cases,]
    
    # scaling
    scale_factors = apply(X_train, 2, sd)
    X_train_sc = scale(X_train, scale=scale_factors)
    # scale the test set features using the same scale factors
    X_test_sc = scale(X_test, scale=scale_factors)
    
    
    # Fit the KNN model (notice the odd values of K)
    knn_K = FNN::knn.reg(train=X_train_sc, test= X_test_sc, y = y_train, k=K)
    ypred_knn = knn_K$pred
    rmse(y_test, ypred_knn)
  }
  colMeans(rmse_vals_K)
}

# Setup for KNN: change from string to numeric
temp = model.matrix( ~ heating-1,SaratogaHouses)
colnames(temp)<- c("heatinghotair", "heatingwatersteam", "heatingelectric")
SaratogaHouses = cbind(SaratogaHouses,temp)
temp = model.matrix( ~ fuel-1,SaratogaHouses)
SaratogaHouses = cbind(SaratogaHouses,temp)
temp = model.matrix( ~ centralAir-1,SaratogaHouses)
SaratogaHouses = cbind(SaratogaHouses,temp)

data = SaratogaHouses

n = nrow(SaratogaHouses)
Ntimes = 100
# performance check
rmse_vals = do(Ntimes)*{
  # re-split into train and test cases
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  DF_train = data[train_cases,]
  DF_test = data[test_cases,]
  
  # fit to this training set
  lm_result1 = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=DF_train)
  lm_result2 = lm(price ~ lotSize + age + livingArea + pctCollege*fireplaces + bedrooms  + bathrooms + rooms + heating + centralAir, data=DF_train)
  lm_result3 = lm(price ~ landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir, data=DF_train)
  lm_result4 = lm(price ~ landValue + livingArea + bedrooms + bathrooms + rooms + heating + fuel + centralAir, data=DF_train)
  # the best regression
  lm_result5 = lm(price ~ landValue + lotSize*(bedrooms + bathrooms) + livingArea*(fuel+ heating + centralAir) + pctCollege*(fireplaces+age) + rooms, data=DF_train)
  lm_result6 = lm(price ~ landValue + lotSize*bedrooms + livingArea*fuel + pctCollege*(fireplaces+age) + bathrooms + rooms + heating + centralAir, data=DF_train)
  lm_result7 = lm(price ~ landValue + lotSize*bedrooms + livingArea*fuel + pctCollege*(fireplaces+age) + centralAir, data=DF_train)
  
  # predict on this testing set
  yhat_test1 = predict(lm_result1, DF_test)
  yhat_test2 = predict(lm_result2, DF_test)
  yhat_test3 = predict(lm_result3, DF_test)
  yhat_test4 = predict(lm_result4, DF_test)
  yhat_test5 = predict(lm_result5, DF_test)
  yhat_test6 = predict(lm_result6, DF_test)
  yhat_test7 = predict(lm_result7, DF_test)
  
  c(rmse(unlist(DF_test$price), yhat_test1), rmse(unlist(DF_test$price), yhat_test2), rmse(unlist(DF_test$price), yhat_test3), rmse(unlist(DF_test$price), yhat_test4), rmse(unlist(DF_test$price), yhat_test5), rmse(unlist(DF_test$price), yhat_test6), rmse(unlist(DF_test$price), yhat_test7))
}
```

Long story short, we "hand-build" a model for price that outperforms the "medium" model that we considered in class, and the KNN model did not beat the “hand-build” model that we selected.

The models that we checked are listed below.

```{r model2.1, echo=TRUE, warning=FALSE}
model1 = price ~ lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir
model2 = price ~ lotSize + age + livingArea + pctCollege*fireplaces + bedrooms  + bathrooms + rooms + heating + centralAir
model3 = price ~ landValue + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir
model4 = price ~ landValue + livingArea + bedrooms + bathrooms + rooms + heating + fuel + centralAir
model5 = price ~ landValue + lotSize*(bedrooms + bathrooms) + livingArea*(fuel+ heating + centralAir) + pctCollege*(fireplaces+age) + rooms
model6 = price ~ landValue + lotSize*bedrooms + livingArea*fuel + pctCollege*(fireplaces+age) + bathrooms + rooms + heating + centralAir
model7 = price ~ landValue + lotSize*bedrooms + livingArea*fuel + pctCollege*(fireplaces+age) + centralAir
```

The performance of the models are mesured with average out-of-sample RMSE. We use the 80% of the data to do regressions and calculate RMSE for the rest 20%, and rerun the Monte Carlo training-testing split to calculate the average RMSE. The result of the models are listed below.

```{r table_2.1.1, echo=FALSE, warning=FALSE}
colnames(rmse_vals) <- c("model 1", "model 2", "model 3", "model 4", "model 5", "model 6", "model 7")
kable(colMeans(rmse_vals), col.names = c("AVG RMSE"), caption = "Table 2.1.1: The Average RMSE for Different Models",padding = 2, align = "c")
```

The best model that we solved is model 5. This model beats all the other models that we choose by having a smaller average RMSE of around 59900, while the average RMSE of the baseline model is around 66000. The regression result is:

```{r table_2.1.2, echo=FALSE, warning=FALSE}
table1 = summary(lm(price ~ landValue + 
                      lotSize*(bedrooms + bathrooms) + 
                      livingArea*(fuel+ heating + centralAir) + 
                      pctCollege*(fireplaces+age) + 
                      rooms, data=data))

kable(as.data.frame(table1["coefficients"]), caption = "Table 2.1.2: Regression Result for the Best Model",padding = 2, align = "c")
```

From the regression we can find many factors that could influence the house price.

First of all, the most important factor is the land value. When we include the land value into account, the RMSE dropped dramatically. It is clearly true that the higher the land value is, the higher the house price is.

Second, the room with more bedrooms have a lower price. This makes sense because more bedrooms means there are more people sharing the apartment, hence the utility of each person is dropping. On the other hand, the apartment with more bathrooms have a higher price. The apartment with larger living area have a higher price, and the price of the apartment drops as the age of the apartment grows. These conclusions matches our intuition.

Third, comparing with the apartments fueling with gas, the apartments fueling with oil have higher prices. comparing with the apartments having a central air conditioner, the apartments with separate air conditioner have higher price. What’s more, the apartments with a fireplace is more expensive, and the older the apartment is the lower the price is.

Forth, the interactions between the number of bedrooms or bathrooms and the lotsize are not significant, but the interactions between the living area and the fuel and whether the house has a central air conditioner is significant. To be more specific, the effect of the living area on the price of the room fueled with oil is lower than the room fueled with gas. And it is also true that the effect of the living area on the price of the room without a central air conditioner is lower than the room that has a central air conditioner.The interactions between whether the apartment is close to a college and the age of the house and the number of fireplaces are significant.

However, by using exactly same variables in the linear regression model it is very hard to beat the linear regression with the KNN model.

```{r graph2.1.1, echo=FALSE, warning=FALSE}

X = subset(SaratogaHouses, select=c(-price,-waterfront,-sewer,-newConstruction,-heating, -fuel, -centralAir, -centralAirNo, -fuelgas, -heatinghotair))
y = subset(SaratogaHouses, select=c(price))

k_grid = seq(3, 30, by=1)

data_X = X
data_y = y
Ntimes = 200
n = nrow(data_X)
KNN_result = do(Ntimes)*{
  # re-split into train and test cases
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  X_train = data_X[train_cases,]
  X_test = data_X[test_cases,]
  y_train = data_y[train_cases,]
  y_test = data_y[test_cases,]
  
  # scaling
  scale_factors = apply(X_train, 2, sd)
  X_train_sc = scale(X_train, scale=scale_factors)
  # scale the test set features using the same scale factors
  X_test_sc = scale(X_test, scale=scale_factors)
  temp = c()
  for(v in k_grid){
    # Fit the KNN model (notice the odd values of K)
    knn_K = FNN::knn.reg(train=X_train_sc, test= X_test_sc, y = y_train, k=v)
    ypred_knn = knn_K$pred
    temp = c(temp, rmse(y_test, ypred_knn))
  }
  temp
}
KNN_result =  KNN_result %>% colMeans() %>% as.data.frame()
colnames(KNN_result) <- c("rmse")
myL = c(3:30)
ggplot(data = KNN_result, aes(x = c(3:30), y = rmse)) + 
  geom_point(shape = "O") +
  geom_line(col = "red")+
  theme_bw()+
  geom_vline(xintercept = myL[which.min(KNN_result$rmse)])+
  labs(title = "AVG RMSE vs K (2<K<30)",
       x = "K",
       y = "AVG RMSE")+
  theme(plot.title = element_text(hjust = 0.5))
```

From the plot of average RMSE vs K we can tell the optimal K for the KNN model is about 10-12. The lowest average RMSE that we can get with KNN model is about 63000, which is still higher than the average RMSE that we conclude using a linear model with interactions.

## Exercise 2.2

```{r setup_2.2, echo=FALSE , warning=FALSE}
urlfile<-'https://raw.githubusercontent.com/znzhao/ECO395M-HW-zz5339/master/HW2/brca.csv'
brca<-read.csv(url(urlfile))
n = nrow(brca)

# define the deviation function as the judgement
dev_out = function(ml, y, dataset) {
  probhat = predict(ml, newdata=dataset, type='response')
  p0 = 1-probhat
  phat = data.frame(P0 = p0, p1 = probhat)
  rc_pairs = cbind(seq_along(y), y)
  -2*sum(log(phat[rc_pairs]))
}
```

### Exercise 2.2.1

First, in order to get the most accurate predictions of radiologists’ recall rates, we construct three logistic models. Because we need to take consideration that all the risk factors need to be constant, we choose age, history of breast biopsy/surgery, breast cancer symptom, menopause/hormone-therapy status and breast density classification as independent variables and recall as dependent variable in our logistic model. After considering the chance of interactions, we selected three following models:

```{r model2.2.1,1, echo=TRUE , warning=FALSE}
model1 = recall~radiologist*(age+history+symptoms+menopause+density)
model2 = recall~radiologist+age+history+symptoms+menopause+density
model3 = recall~(radiologist+age+history+symptoms+menopause+density)^2
```

The performance of the models are measured with error rate, which is calculated by sum of the right diagonal numbers of confusion matrix divided by 987 screening mammograms. We use the 80% of the data to do logistic regressions and calculate error rate for the rest 20%, and rerun the Monte Carlo training-testing split to calculate the average error rate. The threshold that we used for the calculation is about 0.1509, which is the average rate for the recalled patients. The result of the models are listed below:

```{r setup_2.2.1,1, echo=FALSE , warning=FALSE}
data = brca
n = nrow(data)
Ntimes = 100
threshold = sum(brca$recall == 1)/n+0.001

# performance check
rmse_vals = do(Ntimes)*{
  # re-split into train and test cases
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  DF_train = data[train_cases,]
  DF_test = data[test_cases,]
  
  # fit to this training set
  glm_result1 = glm(recall~radiologist*(age+history+symptoms+menopause+density), data=DF_train, family=binomial)
  glm_result2 = glm(recall~radiologist+age+history+symptoms+menopause+density, data=DF_train, family=binomial)
  glm_result3 = glm(recall~(radiologist+age+history+symptoms+menopause+density)^2, data=DF_train, family=binomial)
  
  # predict on this testing set
  phat_test1 = predict(glm_result1, DF_test, type='response')
  yhat_test1 = ifelse(phat_test1 > threshold, 1, 0)
  e1 = sum(yhat_test1 != unlist(DF_test$recall))/n_test
  # predict on this testing set
  phat_test2 = predict(glm_result2, DF_test, type='response')
  yhat_test2 = ifelse(phat_test2 > threshold, 1, 0)
  e2 = sum(yhat_test2 != unlist(DF_test$recall))/n_test
  # predict on this testing set
  phat_test3 = predict(glm_result3, DF_test, type='response')
  yhat_test3 = ifelse(phat_test3 > threshold, 1, 0)
  e3 = sum(yhat_test3 != unlist(DF_test$recall))/n_test 
  c(e1,e2,e3)
}
colnames(rmse_vals) <- c("model 1", "model 2", "model 3")
kable(colMeans(rmse_vals), col.names = c("AVG RMSE"), caption = "Table 2.2.1: The Average RMSE for Different Models",padding = 2, align = "c")
```

After running for several times, we found the error rate of model 1 and model 3 is significantly smaller than that of model 2. We use model 1 to predict in the following as the error rate of model 1 is slightly smaller than that of model 3. 

Then, we randomly chosen 100 samples, which consist of around 10% of 987 screening mammograms. After that, we repeat these 100 samples for 5 times each. In order to address the problem that the radiologists don’t see the same patients, we added an additional row, which is  arranged by repeated series radiologist13, radiologist34, radiologist66, radiologist89 and radiologist95. Therefore, we let five radiologists see the same patients. Finally, we predict the recall rates of each radiologist with model 1. The results is below.

```{r setup_2.2.1.2, echo=FALSE , warning=FALSE}
modelselect=glm(recall~radiologist*(age+history+symptoms+menopause+density),data=brca)
table2_1 = summary(modelselect)
#kable(as.data.frame(table2_1["coefficients"]), caption = "Table 2.2.2: Regression Result for the Best Model",padding = 2, align = "c")# generate the testing set
n = nrow(brca)
# get the sample
pretest_cases = sample.int(n,50,replace=FALSE)
brca_pretest = brca[pretest_cases,]
brca_sample=data.frame(brca_pretest)
# replicate the data for 5 times
brca_samplerepeat=brca_sample[rep(1:nrow(brca_sample),each=5),-1]
# assign the same patient to different doctors
brca_samplerepeat$radiologist=c("radiologist13","radiologist34","radiologist66","radiologist89","radiologist95")
# get diagnose from doctors 
yhat_recall = predict(modelselect, brca_samplerepeat)
brca_samplerepeat=cbind(brca_samplerepeat,yhat_recall)
# compare doctors 
brca_predict<-brca_samplerepeat%>%
group_by(radiologist)%>%
summarise(Prob_recall = mean(yhat_recall))
kable(brca_predict)
```

From the above table, we can clearly see that radiologist89 is most clinically conservative, whose recall rate is about 0.21. Radiologist66, radiologist13, radiologist95 and radiologist34, ranked 2nd, 3rd, 4th and 5th respectivelly in terms of clinically conservative index.

At last, we performed robust test on our results. We predicted recall rates by using model 2 and model 3. The below tables showed the results, which is consistent with the result predicted by model 1.

```{r setup_2.2.1.3, echo=FALSE , warning=FALSE}
modelselect=glm(recall~radiologist+age+history+symptoms+menopause+density,data=brca)
#kable(as.data.frame(table2_1["coefficients"]), caption = "Table 2.2.2: Regression Result for the Best Model",padding = 2, align = "c")# generate the testing set
n = nrow(brca)
# get the sample
pretest_cases = sample.int(n,100,replace=FALSE)
brca_pretest = brca[pretest_cases,]
brca_sample=data.frame(brca_pretest)
# replicate the data for 5 times
brca_samplerepeat=brca_sample[rep(1:nrow(brca_sample),each=5),-1]
# assign the same patient to different doctors
brca_samplerepeat$radiologist=c("radiologist13","radiologist34","radiologist66","radiologist89","radiologist95")
# get diagnose from doctors 
yhat_recall = predict(modelselect, brca_samplerepeat)
brca_samplerepeat=cbind(brca_samplerepeat,yhat_recall)
# compare doctors 
brca_predict2<-brca_samplerepeat%>%
group_by(radiologist)%>%
summarise(Prob_recall = mean(yhat_recall))


modelselect=glm(recall~(radiologist+age+history+symptoms+menopause+density)^2,data=brca)
#kable(as.data.frame(table2_1["coefficients"]), caption = "Table 2.2.2: Regression Result for the Best Model",padding = 2, align = "c")# generate the testing set
n = nrow(brca)
# get the sample
pretest_cases = sample.int(n,50,replace=FALSE)
brca_pretest = brca[pretest_cases,]
brca_sample=data.frame(brca_pretest)
# replicate the data for 5 times
brca_samplerepeat=brca_sample[rep(1:nrow(brca_sample),each=5),-1]
# assign the same patient to different doctors
brca_samplerepeat$radiologist=c("radiologist13","radiologist34","radiologist66","radiologist89","radiologist95")
# get diagnose from doctors 
yhat_recall = predict(modelselect, brca_samplerepeat)
brca_samplerepeat=cbind(brca_samplerepeat,yhat_recall)
# compare doctors 
brca_predict3<-brca_samplerepeat%>%
group_by(radiologist)%>%
summarise(Prob_recall = mean(yhat_recall))
t1 = data.frame(brca_predict2,brca_predict3)
print("model 2")
kable(brca_predict2)
print("model 3")
kable(brca_predict3)
```

In conclusion, holding patient risk factors equal, the order of clinically conservative characteristic in recalling patients is: radiologist89 > radiologist66 > radiologist13 > radiologist95 > radiologist34, when letting radiologists see the same patients.

### Exercise 2.2.2

```{r setup_2.2.2, echo=FALSE , warning=FALSE}
threshold = sum(brca$cancer == 1)/n+0.001
```

The second point that we want to make is that when the radiologists at this hospital interpret a mammogram to make a decision on whether to recall the patient, they should be weighing the history of the patients, the breast cancer symptoms and the menopause status of the patient more heavily than they currently are. Although this means they have to recall more patients back for further checks, it will minimize the false negative rate, identifying more precisely the patients who do end up getting cancer, so that they can be treated as early as possible.

First we build the baseline model, which suggests that when the doctors recall a patient, they tend to think that the patient has a cancer.

To formalize the model by regression, we regressed the patient’s cancer outcome on the radiologist’s recall decision with the logistic regression. The regression model is:

```{r model2.2.1, echo = TRUE, warning=FALSE}
baseline = cancer ~ recall
```
We split the dataset into the training set and the testing set using the standard 80-20 rule, and re-run the regression for 100 times to eliminate the stochasticity, and ending up with similar rates to the ones calculated with the entire database.

If we build a model using the recall decision and all the other clinical risk factors and it significantly performs better than the baseline model, it means that there are some risk factors that the doctors are missing.

We checked (1) the model regressing cancer indicator on the recall indicator and all the risk factors, (2) the model regressing cancer indicator on the recall indicator and all the risk factors and their interactions (3) two hand-build models. The thresholds that we chose for these models are the same as the baseline model, so that we can compare these models on the same level. The regression models are:

```{r model2.2.2, echo = TRUE, warning=FALSE}
model1 = cancer ~ recall + age + history + symptoms + menopause + density
model2 = cancer ~ recall + (age + history + symptoms + menopause + density)^2
model3 = cancer ~ recall + history + symptoms + menopause
```

Before we show the result of the models, we need to explain the criteria that we use to judge these model. When we try to identify the patient, different kinds of error has different cost. It might not be a big problem if a healthy woman is recalled to do some further test, but it may cause death if the doctor didn’t identify the patients who have cancer. Hence the accurate rate is not the best criteria. Instead, we calculate the deviance of these model, and choose the model with smaller deviance.

The average deviance of the models are listed in the following table:

```{r table2.2.1, echo = FALSE, warning=FALSE}
Err1 = do(50)*{
  data = brca
  n = nrow(data)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  DF_train = data[train_cases,]
  DF_test = data[test_cases,]
  
  glm_modelA = glm(cancer~ recall, data = DF_train)
  glm_modelB = glm(cancer~ .-radiologist, data = DF_train)
  glm_modelC = glm(cancer~ recall + (age + history + symptoms + menopause + density)^2, data = DF_train)
  glm_modelD = glm(cancer~ recall + history + symptoms + menopause, data = DF_train)
  
  c(
    dev_out(ml = glm_modelA,dataset = DF_test,y = DF_test$cancer),
    dev_out(ml = glm_modelB,dataset = DF_test,y = DF_test$cancer),
    dev_out(ml = glm_modelC,dataset = DF_test,y = DF_test$cancer),
    dev_out(ml = glm_modelD,dataset = DF_test,y = DF_test$cancer))
  }

colnames(Err1) <- c("Baseline", "Model 1", "Model 2", "Model 3")
kable(colMeans(Err1), col.names = c("AVG Deviance for Different Models"), caption = "Table 2.2.1: The Average RMSE for Different Models",padding = 2, align = "c")
```

From the table we can tell that the Model 3 has the lowest average deviation, which means we can perform better than the doctors currently do if they give more weight on the terms in Model 3. Overall we can say that the doctors did great jobs at identifying the patients who do get cancer. the drop between Model3 and the baseline is very small.

The logistic regression of model 3 using the whole dataset is shown below:

```{r table2.2.2, echo = FALSE, warning=FALSE}
glm_modelA = glm(cancer ~ recall, data = brca)
phatA = predict(glm_modelA, brca, type='response')
yhatA = ifelse(phatA >= threshold, 1, 0)
t1 = xtabs(~brca$cancer+yhatA) 

glm_modelB = glm(cancer ~ recall + history + symptoms + menopause, data = brca)

phatB = predict(glm_modelB, brca, type='response')
yhatB = ifelse(phatB >= threshold, 1, 0)
t2 = xtabs(~brca$cancer+yhatB)

table2 = summary(glm_modelB)
kable(as.data.frame(table2["coefficients"]), caption = "Table 2.1.2: Regression Result for the Best Model",padding = 2, align = "c")
```

From the regression result we can tell that the doctor should consider more about the patient’s history, the breast cancer symptoms and the menopause status of the patient. More specifically, if a person has the history of having cancer, or she has the breast cancer symptoms, or the hormone-therapy status is unknown, she is more likely to have cancers. This result matches our intuition.

To compare the result, we made some predictions with the baseline model and the model we choose.The threshold of positive prediction is chosen as 0.0395, which is slightly higher than the prior probability of having a cancer. 

The confusion matrix for the baseline model using the entire dataset is:

```{r table2.2.3, echo = FALSE, warning=FALSE}
df = as.data.frame(t1)
t1 = data.frame(yhat0 = c(df$Freq[1],df$Freq[2]),yhat1 = c(df$Freq[3],df$Freq[4]))
row.names(t1) <- c("cancer = 0", "cancer = 1")
kable(t1, caption = "Table 2.1.3: Confusion Matrix of the Baseline Model",padding = 2, align = "c", col.names = c("prediction = 0", "prediction = 1"))
```

The accuracy rate is (824+22)/987 = 85.7%, the false negative rate is 15/(22+15) = 40.5%,  the false positive rate is 126/(126+22) = 85.1%.

The confusion matrix for the model using the entire dataset is:

```{r table2.2.4, echo = FALSE, warning=FALSE}
df = as.data.frame(t2)
t2 = data.frame(yhat0 = c(df$Freq[1],df$Freq[2]),yhat1 = c(df$Freq[3],df$Freq[4]))
row.names(t2) <- c("cancer = 0", "cancer = 1")
kable(t2, caption = "Table 2.1.4: Confusion Matrix of the Improved Model",padding = 2, align = "c", col.names = c("prediction = 0", "prediction = 1"))
```

The accuracy rate is (797+23)/987 = 83.1%, the false negative rate is 14/(23+14) = 37.8%, the false positive rate is 153/(153+23) = 86.9%.

Although this is the insample rates, we can still conclude that the false negative rate is decreasing, which means it will lower the false negative rate, identifying more precisely the patients who do end up getting cancer, so that they can be treated as early as possible, while the false positive rate is slightly increasing, meaning the doctors have to be more conservative and hence slightly increase the rate of the false alert. However, clearly the fact that we identified more cancer patients matters more.

## Exercise 2.3

```{r setup2.3, echo = FALSE, warning=FALSE}
urlfile<-'https://raw.githubusercontent.com/znzhao/ECO395M-HW-zz5339/master/HW2/online_news.csv'
OnlineNews<-read.csv(url(urlfile))
OnlineNews$viral = ifelse(OnlineNews$shares > 1400, 1, 0)
```

The goal of this question is to build a model to predict whether an online article goes viral or not. Also from the model we would like to know how to improve an article’s chance of reaching Mashable’s cutoff threshold, 1,400 shares. 

### Benchmark

First we set up our baseline model for predicting which articles go viral. Out of the 39,644 articles, 19,562 of them have gone viral. This means that even if we blindly predict all articles do not go viral, the accuracy rate would reach 50.7%. We will set this number as our baseline accuracy rate and attempt to improve it as much as we can.

### Model Construction
There are 2 approaches to building forecasting model, 1. Predict the number of shares first then classify if each article goes viral by comparing to the threshold of 1,400 shares. 2. Classify the the viral status first by comparing each article to the threshold of 1,400 shares, then directly predict viral status as the target variable. 

For the first approach, we used linear modeling, with different combinations of interaction terms, polynomial terms, and transformations. The improvement in accuracy rate is rather minimal, as some of our models shown below. (Each of the variations)

```{r model2.3.1, echo = TRUE, warning=FALSE}
share_LM1 = shares ~ n_tokens_title + n_tokens_content + num_hrefs + 
  num_self_hrefs + num_imgs + num_videos + 
  average_token_length + num_keywords + data_channel_is_lifestyle +
  data_channel_is_entertainment + data_channel_is_bus + 
  data_channel_is_socmed + data_channel_is_tech +
  data_channel_is_world + self_reference_avg_sharess +
  weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
  weekday_is_thursday + weekday_is_friday + weekday_is_saturday
share_LM2 = shares ~ (n_tokens_title + n_tokens_content + num_hrefs +
                        num_self_hrefs + num_imgs + num_videos + 
                        average_token_length + num_keywords + data_channel_is_lifestyle + 
                        data_channel_is_entertainment + data_channel_is_bus +
                        data_channel_is_socmed + data_channel_is_tech +
                        data_channel_is_world + self_reference_avg_sharess + 
                        weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday +
                        weekday_is_thursday + weekday_is_friday + weekday_is_saturday)^2
share_LM3 = shares ~ poly(n_tokens_title, 3) + poly(num_hrefs, 2) + poly(num_imgs, 2) + poly(num_videos, 2) +
  poly(average_token_length, 3) + poly(num_keywords, 2) + poly(n_tokens_content, 2) +
  data_channel_is_lifestyle + data_channel_is_entertainment + data_channel_is_bus + 
  data_channel_is_socmed + data_channel_is_tech +
  data_channel_is_world + poly(self_reference_avg_sharess,2) + 
  weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
  weekday_is_thursday + weekday_is_friday + weekday_is_saturday + 
  poly(max_positive_polarity, 3) + poly(max_negative_polarity, 3)
share_LM4 = shares ~ n_tokens_title + n_tokens_content + num_hrefs + 
  num_self_hrefs + num_imgs + num_videos + 
  average_token_length + num_keywords + data_channel_is_lifestyle +
  data_channel_is_entertainment + data_channel_is_bus + 
  data_channel_is_socmed + data_channel_is_tech + 
  data_channel_is_world + self_reference_avg_sharess +
  weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
  weekday_is_thursday + weekday_is_friday + weekday_is_saturday

```

The average accurate rate for these for model is listed in the following table.

```{r table2.3.1, echo = FALSE, warning=FALSE}
data = OnlineNews
Ntimes = 100
n = nrow(data)

accrate = do(Ntimes)*{
  # re-split into train and test cases
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  DF_train = data[train_cases,]
  DF_test = data[test_cases,]
  y_test = data$viral[test_cases]
  
  # lm test1
  lm_result1 = lm(shares ~ n_tokens_title + n_tokens_content + num_hrefs + 
                     num_self_hrefs + num_imgs + num_videos + 
                     average_token_length + num_keywords + data_channel_is_lifestyle + 
                     data_channel_is_entertainment + data_channel_is_bus + 
                     data_channel_is_socmed + data_channel_is_tech + 
                     data_channel_is_world + self_reference_avg_sharess + 
                     weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
                     weekday_is_thursday + weekday_is_friday + weekday_is_saturday, data=DF_train)

  sharehat_testlm1 = predict(lm_result1, DF_test)
  yhat_testlm1 = ifelse((sharehat_testlm1>1400), yes = 1 ,no = 0)
  lmar1 = sum(yhat_testlm1 == y_test)/n_test
  
    # lm test2
  lm_result2 = lm(shares ~ (n_tokens_title + n_tokens_content + num_hrefs + 
                                num_self_hrefs + num_imgs + num_videos + 
                                average_token_length + num_keywords + data_channel_is_lifestyle + 
                                data_channel_is_entertainment + data_channel_is_bus + 
                                data_channel_is_socmed + data_channel_is_tech + 
                                data_channel_is_world + self_reference_avg_sharess + 
                                weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
                                weekday_is_thursday + weekday_is_friday + weekday_is_saturday)^2, data=DF_train)

  sharehat_testlm2 = predict(lm_result2, DF_test)
  yhat_testlm2 = ifelse((sharehat_testlm2>1400), yes = 1 ,no = 0)
  lmar2 = sum(yhat_testlm2 == y_test)/n_test
  
    # lm test3
  lm_result3 = lm(shares ~ poly(n_tokens_title, 3) + poly(num_hrefs, 2) + poly(num_imgs, 2) + poly(num_videos, 2) + 
                      poly(average_token_length, 3) + poly(num_keywords, 2) + poly(n_tokens_content, 2) + 
                      data_channel_is_lifestyle + data_channel_is_entertainment + data_channel_is_bus + 
                      data_channel_is_socmed + data_channel_is_tech + 
                      data_channel_is_world + poly(self_reference_avg_sharess,2) + 
                      weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
                      weekday_is_thursday + weekday_is_friday + weekday_is_saturday + 
                      poly(max_positive_polarity, 3) + poly(max_negative_polarity, 3), data=DF_train)

  sharehat_testlm3 = predict(lm_result3, DF_test)
  yhat_testlm3 = ifelse((sharehat_testlm3>1400), yes = 1 ,no = 0)
  lmar3 = sum(yhat_testlm3 == y_test)/n_test
  
    # lm test4
  lm_result4 = lm(log(shares) ~ poly(n_tokens_title, 3) + poly(num_hrefs, 2) + poly(num_imgs, 2) + poly(num_videos, 2) + 
                           poly(average_token_length, 3) + poly(num_keywords, 2) + poly(n_tokens_content, 2) + 
                           data_channel_is_lifestyle + data_channel_is_entertainment + data_channel_is_bus + 
                           data_channel_is_socmed + data_channel_is_tech + 
                           data_channel_is_world + poly(self_reference_avg_sharess,2) + 
                           weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
                           weekday_is_thursday + weekday_is_friday + weekday_is_saturday + 
                           poly(max_positive_polarity, 3) + poly(max_negative_polarity, 3), data=DF_train)

  sharehat_testlm4 = predict(lm_result4, DF_test)
  yhat_testlm4 = ifelse((sharehat_testlm4>log(1400)), yes = 1 ,no = 0)
  lmar4 = sum(yhat_testlm4 == y_test)/n_test
  
  # lm-viral test
  lm_result = lm(viral ~ n_tokens_title + n_tokens_content + num_hrefs + 
                     num_self_hrefs + num_imgs + num_videos + 
                     average_token_length + num_keywords + data_channel_is_lifestyle + 
                     data_channel_is_entertainment + data_channel_is_bus + 
                     data_channel_is_socmed + data_channel_is_tech + 
                     data_channel_is_world + self_reference_avg_sharess + 
                     weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
                     weekday_is_thursday + weekday_is_friday + weekday_is_saturday, data=DF_train)

  phat_test = predict(lm_result, DF_test)
  yhat_test2 = ifelse((phat_test>0.5), yes = 1 ,no = 0)
  ar2 = sum(yhat_test2 == y_test)/n_test
  
  # glm test
  glm_result = glm(viral ~ n_tokens_title + n_tokens_content + num_hrefs + 
                       num_self_hrefs + num_imgs + num_videos + 
                       average_token_length + num_keywords + data_channel_is_lifestyle + 
                       data_channel_is_entertainment + data_channel_is_bus + 
                       data_channel_is_socmed + data_channel_is_tech + 
                       data_channel_is_world + self_reference_avg_sharess + 
                       weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
                       weekday_is_thursday + weekday_is_friday + weekday_is_saturday, data=DF_train, family=binomial)
  
    phat_test3 = predict(glm_result, DF_test, type='response')
    yhat_test3 = ifelse(phat_test3 > 0.5, 1, 0)
    ar3 = sum(yhat_test3 == unlist(DF_test$viral))/n_test 
    
    tpr_lm4 = sum(yhat_testlm4 == 1 & unlist(DF_test$viral) == 1)/sum(unlist(DF_test$viral) == 1)
    fpr_lm4 = sum(yhat_testlm4 == 1 & unlist(DF_test$viral) == 0)/sum(unlist(DF_test$viral) == 0)
    
    tpr3 = sum(yhat_test3 == 1 & unlist(DF_test$viral) == 1)/sum(unlist(DF_test$viral) == 1)
    fpr3 = sum(yhat_test3 == 1 & unlist(DF_test$viral) == 0)/sum(unlist(DF_test$viral) == 0)
    
  c(lmar1,lmar2,lmar3,lmar4, ar2, ar3, tpr_lm4, fpr_lm4, tpr3, fpr3)
  }
colnames(accrate) <- c("share-LM1","share-LM2","share-LM3","share-LM4", "viral-LM", "GLM", "tpr_LM", "fpr_LM", "tpr_GLM", "fpr_GLM")
kable(colMeans(accrate)[1:4], col.names = "Accurate Rate")
```

All the results above are the average of running against 100 random train/test splits.

Share-LM4 had the highest accuracy rate, so we report the confusion matrix. The following confusion matrix consists the testing data result from one iteration. In addition to the accuracy rate, we calculated the overall error rate, true positive rate and false positive rate in 100 iterations and took the average for each.

```{r table2.3.1.1, echo = FALSE, warning=FALSE}
t = xtabs(~DF_test$viral+yhat_testlm4)
df = as.data.frame(t)
t = data.frame(yhat0 = c(df$Freq[1],df$Freq[2]),yhat1 = c(df$Freq[3],df$Freq[4]))
row.names(t) <- c("viral = 0", "viral = 1")
kable(t, padding = 2, align = "c", col.names = c("prediction = 0", "prediction = 1"))
```

Next, we moved on to the second approach. As the the viral status is binomial, in addition to the linear modelling, we were also able to exploit the logit model. After conducting trial and error with different forms of explanatory variables as in the first approach, the highest accuracy rate with the best configurations are shown below.

```{r table2.3.1.2, echo = FALSE, warning=FALSE}
kable(colMeans(accrate)[5:6], col.names = "Accurate Rate")
```

All the results above are the average of running against 100 random train/test splits.

The second approach turned out to be better, providing us with higher accuracy rate. In addition, as a supplement, we also explored the KNN method. The result was similar to the linear/logit models, as shown below.

```{r table2.3.3, echo = FALSE, warning=FALSE}

K = 20
run_time = 5
myList <- c(rep(0, 20))
for(i in c(1:run_time)){
  n = nrow(OnlineNews)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  OnlineNews_train = OnlineNews[train_cases,]
  OnlineNews_test = OnlineNews[test_cases,]
  # training and testing set responses
  ytrain = OnlineNews_train$viral
  share_train =  OnlineNews_train$shares
  ytest = OnlineNews_test$viral
  Xtrain = model.matrix(~ n_tokens_title + n_tokens_content + num_hrefs + 
                          num_self_hrefs + num_imgs + num_videos + 
                          average_token_length + num_keywords + data_channel_is_lifestyle + 
                          data_channel_is_entertainment + data_channel_is_bus + 
                          + data_channel_is_socmed + data_channel_is_tech + 
                          data_channel_is_world + self_reference_avg_sharess + 
                          weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
                          weekday_is_thursday + weekday_is_friday + weekday_is_saturday - 1, data=OnlineNews_train)
  Xtest = model.matrix(~ n_tokens_title + n_tokens_content + num_hrefs + 
                         num_self_hrefs + num_imgs + num_videos + 
                         average_token_length + num_keywords + data_channel_is_lifestyle + 
                         data_channel_is_entertainment + data_channel_is_bus + 
                         + data_channel_is_socmed + data_channel_is_tech + 
                         data_channel_is_world + self_reference_avg_sharess + 
                         weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + 
                         weekday_is_thursday + weekday_is_friday + weekday_is_saturday - 1, data=OnlineNews_test)
  # now rescale:
  scale_train = apply(Xtrain, 2, sd)  # calculate std dev for each column
  Xtilde_train = scale(Xtrain, scale = scale_train)
  Xtilde_test = scale(Xtest, scale = scale_train)  # use the training set scales!
  for(j in c(1:K)){
    m = j*5
    # KNN - viral
    knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=m)
    KNN_train_test1 = ifelse(knn_model$pred > 0.5, 1, 0)
    confusion_in_KNN = table(y = OnlineNews_test$viral, yhat = KNN_train_test1)
    accuracy_rate = sum(diag(confusion_in_KNN))/sum(confusion_in_KNN)
    myList[j]= myList[j] + accuracy_rate
    }
  # fit the model

}
myList = myList/run_time
myL <- seq(5,100, by=5)
accrate <- data.frame(myList, myL)
ggplot(data = accrate, aes(x = myL, y = myList))+
  geom_point(shape = "O")+
  geom_line(col = "red")+
  theme_bw()+
  geom_vline(xintercept = myL[which.max(myList)])+
  labs(title = "AVG Accuracy Rate vs K (5<K<100) Model: KNN classification",
       x = "K",
       y = "AVG Accuracy Rate")+
  theme(plot.title = element_text(hjust = 0.5))

```

the result above are the average of running 5 train/test splits iterations.

We now report the confusion matrix with the logit model which provided us the highest accuracy rate. The following confusion matrix consists the testing data result in one iteration. In addition to the accuracy rate, we calculated the overall error rate, true positive rate and false positive rate in 100 iterations and took the average for each. 

```{r table2.3.2, echo = FALSE, warning=FALSE}
t = xtabs(~DF_test$viral+yhat_test3)
df = as.data.frame(t)
t = data.frame(yhat0 = c(df$Freq[1],df$Freq[2]),yhat1 = c(df$Freq[3],df$Freq[4]))
row.names(t) <- c("viral = 0", "viral = 1")
kable(t,padding = 2, align = "c", col.names = c("prediction = 0", "prediction = 1"))
```

### Conclusion

The second approach, threshold first and regress/classify second, performed better than the first one with log transformation which was just under 60%. Our logit model reached 62.7% accuracy rate, approximately 23.7% (0.627/0.507 - 1) improvement relative to our benchmark. 

One possibility that our first approach performed worse is that our explanatory variables are not good at creating a low variance linear relationship to predict numerical amount of shares. Under similar set of conditions, articles range high in both >1,400 and <1,400, resulting low accuracy rate. Once we take the log transformation of shares, variance goes down, providing us a better result . On the other hand, after we first classified articles into a binary variable, viral and non-viral, the variance in predicting numerical number of shares are eliminated, so that our explanatory variables seem to be relatively better at predicting how likely each article goes viral at 50% mark (>50% then predict viral, <50% then predict non-viral). 

