---
title: "Exercise 1"
author: "Chong Wang, Tianping Wu, Zhenning Zhao"
date: "2019/2/9"
output:
  md_document:
    variant: markdown_github
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(RColorBrewer)
library(FNN)
library(maps)
library(gganimate)
library(gifski)
```

# Exercise 1
By Chong Wang, Tinaping Wu, Zhenning Zhao

## Exercise 1.1
The environmentally friendly buildings have some obvious advantages, not only from an eco-friendly view, but also from the financial aspect. The "data guru" did not make a reasonable argument to quantity the advantages clearly. To be more specific, we believe that the duration that he/she calculated to recover the cost may be mistaken.

```{r p1.1, echo=FALSE, warning=FALSE, fig.align='center'}
urlfile<-'https://raw.githubusercontent.com/znzhao/ECO395M-HW-zz5339/master/HW1/greenbuildings.csv'
greenbuildings<-read.csv(url(urlfile))
greenbuildings$Green[greenbuildings$green_rating==0] <- "Non-Green"
greenbuildings$Green[greenbuildings$green_rating==1] <- "Green"
# Occupancy Rates & Rent Plot: NonGreen vs Green
NG <- subset(greenbuildings, (greenbuildings$green_rating == "0"))
p1 <- ggplot(data = greenbuildings) + 
  geom_point(mapping = aes(y = Rent, x = leasing_rate),alpha = 0.3, col = brewer.pal(6, "Greens")[5])+
  facet_wrap(~ Green)+
  geom_point(data = NG, mapping = aes(y = Rent, x = leasing_rate), alpha = 0.3, col = brewer.pal(6, "Reds")[5])+
  labs(title = "Occupancy Rate vs Rent", 
       y = "Rent ($/sqft)",
       x = "Occupancy Rate (%)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
p1
```

The first step is to clean the data. The "data guru" noticed that a handful of the buildings in the data set had very low occupancy rates. Although this is a true fact, which can be shown in the graph above, he/she did not provide a satisfactory reason for why the buildings with less than 10% leasing rate should be removed from consideration. Due to this fact, we disagree to simply delete these 215 data points. Instead, we were able to identify a reason to delete buildings with 0% occupancy rate. According to a [research conducted by IBM](https://www.ibm.com/support/knowledgecenter/en/SSFCZ3_10.5.2/com.ibm.tri.doc/wpm_metrics/r_occupancy_rate.html), buildings with a 0% leasing rate is called “not-used buildings”. Since our new building will certainly be used, we can comfortably exclude these data points from the original data set.

After that, we tried to clean the data even furthur. Since this new project is in Austin with the tallest building at 56 floors, clusters with buildings higher than this value are clearly not in Austin and therefore should be removed from the data set. As the result, we are left with 6,618 data points.

```{r p1.2, echo=FALSE, warning=FALSE, fig.align='center'}
# clean data by deleting the data with occupacy rate equal to 0% and taller than 56 floor
GB_test <- subset(greenbuildings,(greenbuildings$stories > 56))
Clus = unique(GB_test$cluster)

GB_cleaned <- subset(greenbuildings,(greenbuildings$leasing_rate != 0))
for (i in 1:39){
  GB_cleaned <-subset(GB_cleaned,(GB_cleaned$cluster!= Clus[i]))
}

# basic boxplot
NG_one <- subset(GB_cleaned, green_rating == "0")
p2 = ggplot(data = GB_cleaned, aes(x = Green, y = Rent)) + 
  geom_boxplot(fill = brewer.pal(6, "Greens")[5], alpha = 0.8)+
  geom_boxplot(data = NG_one, fill = brewer.pal(6, "Reds")[5], alpha= 0.9)+
  theme_bw()+
  labs(title = "Green Rating vs Rent",
       x = "Green Buildings or Not",
       y = "Rent ($/sqft)")+
  theme(plot.title = element_text(hjust = 0.5))
p2
```

According to the "data guru", there are still many outliers after data cleaning, so he/she chooses to use the median instead of the mean to calculate the expected rent to compare green and non-green buildings. We plot the boxplot of green and non-green buildings to get a overview of the data. It seems that the "data guru" was right about the outliers, but it is unclear why there are so many outliers. So we digged deeper, and got some interesting findings.

```{r p1.4, echo=FALSE, warning=FALSE, fig.align='center'}
NG <- subset(GB_cleaned, (GB_cleaned$Green == "Non-Green"))
p4 <- ggplot(data = GB_cleaned)+
  geom_point(mapping = aes(y = stories, x = size),alpha = 0.3, col = brewer.pal(6, "Greens")[5])+
  facet_wrap(~ Green)+
  geom_point(data = NG, mapping = aes(y = stories, x = size), alpha = 0.3, col = brewer.pal(6, "Reds")[5])+
  labs(title = "Size vs Story", 
       x = "Size (sqft)",
       y = "Story")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
p4
```

In order to narrow down existing data set further and only use data points that are relevant and comparable to our project, we looked for market standards to categorize buildings.  According to [the Commercial Real Estate Terms and Definitions](https://www.naiop.org/-/media/Research/Research/Research-Reports/Terms-and-Definitions/CRE-Terms-and-Definitions-2017.ashx?la=en) by The NAIOP Research Foundation, commercial buildings can be divided into 3 types: low rise, mid rise, and high rise. Our 15-story building falls into the mid-rise category. The plot above confirms that story is positively correlated with size in our data set and there are many mid rise buildings with similar attributes that can be used to estimate our building’s rent. 

```{r p1.6, echo=FALSE, warning=FALSE, fig.align='center'}
# Age of the Building
p7 = ggplot() + 
  geom_point(data = GB_cleaned, aes(x = age, y = Rent), alpha =0.3, col = brewer.pal(6, "Greens")[5])+
  geom_point(data = NG, mapping = aes(x = age, y = Rent), alpha = 0.3, col = brewer.pal(6, "Reds")[5])+
  labs(title = "Age vs Rent", x = "Age (year)", y = "Rent ($/sqft)")+
  facet_wrap(~ Green)+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
p7
```

Next, we were curious if building’s age has any impact on rent. As shown in the plot above, rent shows no distinct trend as building’s age goes up, at least up to 100 years. This let us treat all buildings with different age equally for rent estimation.

```{r p1.7, echo=FALSE, warning=FALSE, fig.align='center'}
# category: https://en.wikipedia.org/wiki/List_of_building_types
GB_cleaned$size_category = cut(GB_cleaned$size, breaks = c(0,125000,400000,800000,10000000000))
GB_cleaned$stories_category = cut(GB_cleaned$stories, breaks = c("0","7","25","150"))
GB_cleaned$stories_category <- factor(GB_cleaned$stories_category, labels = c(expression("Low Rise(<7)"), "Mid Rise(7-25)", expression("High Rise(>25)")))
GB_summ <- GB_cleaned %>%
  group_by(amenities,size_category,Green,stories_category) %>%
  summarise(Rent_median = median(Rent), Rent_mean = mean(Rent), leasing_rate = mean(leasing_rate))
GB_summ_a = GB_cleaned %>%
  group_by(amenities,stories_category, Green) %>%
  summarise(Rent_mean = mean(Rent))
p6 = ggplot(GB_summ_a, aes(x=amenities, y=Rent_mean,fill=Green)) + 
  geom_bar(alpha = 0.8, stat='identity',position='dodge')+
  geom_text(aes(x=amenities,y=Rent_mean,label=round(Rent_mean)),vjust=-0.1,col="black",position = position_dodge(0.9))+
  facet_wrap(~ stories_category) +
  scale_x_continuous(breaks = c(0,1),labels = c("0" = "No", "1" = "Yes"))+
  labs(title = "Different Stories and Access of Amenity vs Rent",
       x = "Is Amenity Available?", y = "Rent ($/sqft)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),legend.position="bottom") + 
  scale_fill_manual(name = "Green Buildings or Not", values = c(brewer.pal(6, "Greens")[5],brewer.pal(6, "Reds")[5]))
p6
```

Lastly we added one more dimension to identify data points that are relevant to our building: Amenities. Our building is for mix use with amenities such as on-site bank, convenience store, dry cleaner, restaurant, retail shops, and fitness center. It is reasonable to assume these buildings would set rent differently than ones without amenities, as more value is added.


To our surprise, mid rise and high rise green buildings with amenities actually had higher rent than green buildings without amenities, as shown in the plot above. Putting this strange result aside, we will focus on the green and non-green buildings within the mid-rise group. It turns out that rent for both types are $31 per square foot per year.

```{r p1.8, echo=FALSE, warning=FALSE, fig.align='center'}
# Which occupancy rate should be chosen?

# Stories & Occupancy Rate Plot
  GB_summ2 <- GB_cleaned %>%
  group_by(stories_category,Green) %>%
  summarise(Rent_median = median(Rent), Rent_mean = mean(Rent), leasing_rate = mean(leasing_rate)/100)

p8 = ggplot(GB_summ2,aes(x = stories_category, y = leasing_rate, fill = Green))+
  scale_y_continuous(labels = scales::percent)+
  geom_bar(alpha = 0.8,stat='identity',position= position_dodge())+
  geom_text(aes(x=stories_category,y=leasing_rate,label=scales::percent(round(leasing_rate,2))),vjust=-0.1,col="black",position = position_dodge(0.9))+
  scale_fill_manual(name = "Green Buildings or Not", values = c(brewer.pal(6, "Greens")[5], brewer.pal(6, "Reds")[5]))+
  labs(title = "Story vs Occupancy Rate",
       y = "Occupancy Rate (%)",
       x = "Story")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5), legend.position="bottom")
p8
```

In our final step, we aimed to explore the relationship between stories and occupancy rate, as well as age and occupancy rate in green and non-green buildings separately.

First, using all of occupancy rate data, we compared the difference of occupancy rate between green and non-green buildings, divided by low rise, medium rise and high rise category. By plotting bar charts above, we could clearly see the occupancy rate of green building is higher than non-green building in each height category. Since our 15-story building falls into the mid-rise category, we can see that the occupancy rate of green building is 5% higher than non-green building in the long run. Thus, if we invest a 15-story green building instead of a non-green one, we can expect higher occupancy rate. 

```{r p1.9, echo=FALSE, warning=FALSE, fig.align='center'}
# Occupancy Rate and rich area
GB_summ3 <-GB_cleaned %>%
  group_by(cluster) %>%
  summarise(building_num = length(cluster), leasing_rate = mean(leasing_rate))
GB_summ4 <-GB_cleaned %>%
  group_by(age, Green) %>%
  summarise(leasing_rate = mean(leasing_rate)/100)
GB_summ4$age <- as.numeric(GB_summ4$age)
GB_summ4 <- subset(GB_summ4, (GB_summ4$age<30))

p9 = ggplot(GB_summ4,aes(x = age, y = leasing_rate))+
  scale_y_continuous(labels = scales::percent,limits = c(0,1))+
  geom_point(data = GB_summ4, aes(x = age, y = leasing_rate, col = Green))+
  geom_line(data = GB_summ4, aes(x = age, y = leasing_rate, col = Green))+
  annotate("text", x = -0.1, y = 0.49, label = "47%", col=brewer.pal(6, "Reds")[5])+
  annotate("text", x = -0.1, y = 0.58, label = "56%", col=brewer.pal(6, "Greens")[5])+
  annotate("text", x = 1, y = 0.67, label = "65%", col=brewer.pal(6, "Reds")[5])+
  annotate("text", x = 1, y = 0.81, label = "79%", col=brewer.pal(6, "Greens")[5] )+
  annotate("text", x = 1.9, y = 0.69, label = "67%", col=brewer.pal(6, "Reds")[5])+
  annotate("text", x = 2, y = 0.91, label = "89%", col=brewer.pal(6, "Greens")[5])+
  scale_color_manual(name = "Green Buildings or Not", values = c(brewer.pal(6, "Greens")[5],brewer.pal(6, "Reds")[5]))+
  labs(title = "Age vs Occupancy Rate",
       y = "Occupancy rate (%)",
       x = "Age of the building (year)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5), legend.position="bottom")
p9
```

Then we compared the difference of occupancy rate between green and non-green buildings in different ages. We selected the buildings that are below 30 years. The average occupancy rate of green buildings are almost higher than non-green buildings every year in the first 30 years from the line chart. In the first 5 years, except year 3 and year 4, the occupancy rate of green buildings is significantly below 80%. Therefore, the average occupancy rate in the first ten years will be highly likely lower than 90%, the most pessimistic estimation by the staff. With such low occupancy rate, we will most likley recover the costs longer than 9 years.
Finally, we calculated the duration of payback. We used the function below to calculate the duration. 

<img src="Exercise_1_Report_files/figure-markdown_github/formula.PNG" style="display: block; margin: auto;" />

C represents baseline construction costs. For green building, C is equal to $105 million; for non-green building, C is equal to $100 million. S represents size, which is 250,000 square feet for both green and non-green buildings. R is the average market rent per square foot. According to our result above, the rent of 15-story building, which falls into the mid-rise category, is $31 per square foot per year for both green and non-green buildings. O represents the occupancy rate. The long term occupancy rates for green and non-green building are 91% and 86% respectively. Because occupancy rates for both green and non-green building in the first three years are significantly lower than 90%, we selected start-up occupancy rates for the first three years. Thus, after calculation, the duration of payback for green building is approximately 15.4 years, while the duration for non-green building is about 15.9 years.

Thus, we support to build the green building for its shorter payback duration. This is much longer than the "data guru"'s estimation.

## Exercise 1.2

```{r setup1.2, echo = FALSE, warning=FALSE}
urlfile <- 'https://raw.githubusercontent.com/znzhao/ECO395M-HW-zz5339/master/HW1/ABIA.csv'
ABIA <- read.csv(url(urlfile), stringsAsFactors=FALSE)
urlfile2 = 'https://raw.githubusercontent.com/znzhao/ECO395M-HW-zz5339/master/HW1/airports.csv'
airports <- read.csv(url(urlfile2), stringsAsFactors=FALSE)

myABIA = ABIA
USairports <- subset(airports,(airports$iso_country =="US"))

# Q1: What is the best time of day to fly to minimize delays?
```

In this question, we have the data of all the flight information of the airlines flying in and out of Austin.

### What is the best time of day to fly to minimize delays?

After clearing the data by deleting the cancelled flights and the diverted flights, we end up with a dataset of 97,659 pieces of data which will tell us the information about the delays.

```{r Q1.2.1, echo = FALSE, warning=FALSE}
# Calculate delaying departure time:
myABIA$TWindow = floor(myABIA$DepTime/100)
myABIA$CRSTWindow_D = floor(myABIA$CRSDepTime/100)
myABIA$CRSTWindow_A = floor(myABIA$CRSArrTime/100)
myABIA$INorOUT[myABIA$Dest == "AUS"] = "Arrival"
myABIA$INorOUT[myABIA$Origin == "AUS"] = "Departure"
myABIA_fly <- subset(myABIA,(myABIA$Cancelled == "0"))
myABIA_fly <- subset(myABIA_fly,!(myABIA_fly$Diverted == 1))
myABIA_cancel <- subset(myABIA,(myABIA$Cancelled == "1"))

# plot the summ for all the airline companies 
myABIA_CRSsumm_D_total <- myABIA_fly %>%
  group_by(CRSTWindow_D,INorOUT) %>%
  summarise(DepDelay_mean = mean(DepDelay))

myABIA_CRSsumm_A_total <- myABIA_fly %>%
  group_by(CRSTWindow_A,INorOUT) %>%
  summarise(ArrDelay_mean = mean(ArrDelay))

temp1 <- subset(myABIA_CRSsumm_D_total,(myABIA_CRSsumm_D_total$INorOUT == "Departure"))

p1 = ggplot(data = subset(myABIA_CRSsumm_D_total,(myABIA_CRSsumm_D_total$INorOUT == "Departure")))+
  geom_bar(aes(x = CRSTWindow_D, y = DepDelay_mean),stat='identity',position='dodge')+
  geom_bar(aes(x = temp1$CRSTWindow_D[which.max(temp1$DepDelay_mean)], 
               y = temp1$DepDelay_mean[which.max(temp1$DepDelay_mean)]),stat='identity',position='dodge', fill = "red")+
  labs(title = "CRS Time vs Average Departure Delay", x = "CRS Time", y = "Departure Delay (min)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
p1
```

First, we categorized the data by the scheduled departure time for the flights that depart Austin. And it gave us the above picture. According to the bar plot, the flights that depart Austin at midnight (at 12:00 AM) have the most departure delay. The average delay time for each flight is 23 minutes. It goes well with our intuition, since the ticket for the night flight is usually cheaper, the airline company don’t pay much attention to the schedule. The best time to depart Austin, on the other hand, is in the morning. The flights at 6:00 AM actually departed earlier than the scheduled time.

```{r Q1.2.2, echo = FALSE, warning=FALSE}
temp2 <- subset(myABIA_CRSsumm_A_total,(myABIA_CRSsumm_A_total$INorOUT == "Arrival"))

p2 = ggplot(data = subset(myABIA_CRSsumm_A_total,(myABIA_CRSsumm_A_total$INorOUT == "Arrival")))+
  geom_bar(aes(x = CRSTWindow_A, y = ArrDelay_mean),stat='identity',position='dodge')+
  geom_bar(aes(x = temp2$CRSTWindow_A[which.max(temp2$ArrDelay_mean)], 
               y = temp2$ArrDelay_mean[which.max(temp2$ArrDelay_mean)]),stat='identity',position='dodge', fill = "red")+
  labs(title = "CRS Time vs Average Arrival Delay", x = "CRS Time", y = "Arrival Delay (min)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
p2
```

Second, we categorized the data by the scheduled arrival time for the flights that arrive Austin. And it gave us the above picture. According to the bar plot, the flights that arrive Austin at 10:00 PM have the most arrival delay, and the average delay time for each flight is 18 minutes.The best time to arrive in Austin is in the morning again. The flights at 7:00 AM arrived a little earlier than the scheduled time.

```{r Q1.2.3, echo = FALSE, warning=FALSE}
# Now for different airline companies
# myABIA_CRSsumm_Departure <- subset(myABIA_CRSsumm_Departure,(myABIA_CRSsumm_Departure$UniqueCarrier != "YV"))
myABIA_CRSsumm_Departure <- myABIA_fly %>%
  group_by(CRSTWindow_D,INorOUT,UniqueCarrier) %>%
  summarise(DepDelay_mean = mean(DepDelay))
myABIA_CRSsumm_Departure <- subset(myABIA_CRSsumm_Departure,(myABIA_CRSsumm_Departure$INorOUT == "Departure"))
myABIA_CRSsumm_Arrival <- myABIA_fly %>%
  group_by(CRSTWindow_A,INorOUT,UniqueCarrier) %>%
  summarise(ArrDelay_mean = mean(ArrDelay))
myABIA_CRSsumm_Arrival <- subset(myABIA_CRSsumm_Arrival,(myABIA_CRSsumm_Arrival$INorOUT == "Arrival"))

# calculate the max point for D
Delay_D_max = aggregate(myABIA_CRSsumm_Departure$DepDelay_mean,by=list(name=myABIA_CRSsumm_Departure$UniqueCarrier),FUN=max)
for (i in Delay_D_max$name) {
  x = myABIA_CRSsumm_Departure$CRSTWindow_D[which((myABIA_CRSsumm_Departure$DepDelay_mean == Delay_D_max$x[Delay_D_max$name == i]))]
  Delay_D_max$time[Delay_D_max$name == i] <- x
}
colnames(Delay_D_max)[1:3] <- c("UniqueCarrier", "Vmax", "CRSTWindow_D")
p3 = ggplot()+
  geom_bar(data = myABIA_CRSsumm_Departure, aes(x = CRSTWindow_D, y = DepDelay_mean),stat='identity')+
  facet_wrap(~UniqueCarrier) +
  geom_bar(data = Delay_D_max, aes(x = CRSTWindow_D, y = Vmax),stat='identity', fill = "red")+
  labs(title = "CRS Time vs Average Departure Delay for Different Airline Companies", 
       x = "CRS Time", y = "Departure Delay (min)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
p3
```

After checking the overall data, we seperated the data by airline companies, and gained the bar plots for the departure delay and the arrival delay for each airline company, and the time window in which the flight has the most delay on average is labeled in red. Clearly the YV airlines has the most departure delay at 2:00 PM, which is longer than any other companies’ flight.

```{r Q1.2.4, echo = FALSE, warning=FALSE}
Delay_A_max = aggregate(myABIA_CRSsumm_Arrival$ArrDelay_mean,by=list(name=myABIA_CRSsumm_Arrival$UniqueCarrier),FUN=max)
for (i in Delay_A_max$name) {
  x = myABIA_CRSsumm_Arrival$CRSTWindow_A[which((myABIA_CRSsumm_Arrival$ArrDelay_mean == Delay_A_max$x[Delay_A_max$name == i]))]
  Delay_A_max$time[Delay_A_max$name == i] <- x
}
colnames(Delay_A_max)[1:3] <- c("UniqueCarrier", "Vmax", "CRSTWindow_A")

p4 = ggplot()+
  geom_bar(data = myABIA_CRSsumm_Arrival, aes(x = CRSTWindow_A, y = ArrDelay_mean),stat='identity')+
  facet_wrap(~UniqueCarrier) +
  geom_bar(data = Delay_A_max, aes(x = CRSTWindow_A, y = Vmax),stat='identity', fill = "red")+
  labs(title = "CRS Time vs Average Arrival Delay for Different Airline Companies", 
       x = "CRS Time", y = "Arrival Delay (min)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
p4
```

From the view of arrival delay, the time for delay of OH, OO and UA significantly worse than other companies. The OH flights have worst arrival delay at 7:00PM; The OO flights have worst arrival delay at 9:00PM. And the UA flights have worst arrival delay at 12:00AM.

### What is the best time of year to fly to minimize delays?

Similar to the first part, we checked the best and worst time of the year to fly in or out of Austin. And we end up with the following two bar plots. 

```{r Q1.2.5, echo = FALSE, warning=FALSE}
# Q2: What is the best time of year to fly to minimize delays?
# plot the summ for all the airline companies 
myABIA_summ_Month <- myABIA_fly %>%
  group_by(Month,INorOUT) %>%
  summarise(DepDelay_mean = mean(DepDelay),ArrDelay_mean = mean(ArrDelay))

temp3 = subset(myABIA_summ_Month,(myABIA_summ_Month$INorOUT == "Departure"))

p5 = ggplot(data = subset(myABIA_summ_Month,(myABIA_summ_Month$INorOUT == "Departure")))+
  geom_bar(aes(x = Month, y = DepDelay_mean),stat='identity',position='dodge')+
  geom_bar(aes(x = temp3$Month[which.max(temp3$DepDelay_mean)], 
               y = temp3$DepDelay_mean[which.max(temp3$DepDelay_mean)]),stat='identity',position='dodge', fill = "red")+
  labs(title = "Month vs Average Departure Delay", x = "Month", y = "Departure Delay")+
  theme_bw()+
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12),labels = c("1" = "Jan", "2" = "Feb", "3" = "Mar", "4" = "Apr", "5" = "May", "6" = "Jun", "7" = "Jul", "8" = "Aug", "9" = "Sep", "10" = "Oct", "11" = "Nov", "12" = "Dec"))+
  theme(plot.title = element_text(hjust = 0.5))
p5
```

Clearly, December and March are the two worst months to fly out of Austin. Each flight delays about 13 minutes in December, and 12 minutes in March on average. The best months to fly out of Austin, on the other hand, is September and October.

```{r Q1.2.6, echo = FALSE, warning=FALSE}
temp4 <- subset(myABIA_summ_Month,(myABIA_summ_Month$INorOUT == "Arrival"))

p6 = ggplot(data = subset(myABIA_summ_Month,(myABIA_summ_Month$INorOUT == "Arrival")))+
  geom_bar(aes(x = Month, y = ArrDelay_mean),stat='identity',position='dodge')+
  geom_bar(aes(x = temp4$Month[which.max(temp4$ArrDelay_mean)], 
               y = temp4$ArrDelay_mean[which.max(temp4$ArrDelay_mean)]),stat='identity',position='dodge', fill = "red")+
  labs(title = "Month vs Average Arrival Delay", x = "Month", y = "Arrival Delay (min)")+
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12),labels = c("1" = "Jan", "2" = "Feb", "3" = "Mar", "4" = "Apr", "5" = "May", "6" = "Jun", "7" = "Jul", "8" = "Aug", "9" = "Sep", "10" = "Oct", "11" = "Nov", "12" = "Dec"))+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
p6
```

When we look at the flights arriving in Austin, things did not change much. December and March are still the two worst months to fly to Austin. Each flight delays about 15 minutes in December, and 12 minutes in March on average. The best time to fly out of Austin is still September and October. Hence if you want to visit Austin, try to come in the fall, so that there will be no annoying flight delays!

### How do patterns of flights to different destinations or parts of the country change over the course of the year?

```{r Q1.2.7_setup, echo = FALSE, warning=FALSE}
# Q3: How do patterns of flights to different destinations or parts of the country change over the course of the year?
# first get the location of the Destination and the Origin
USairportLocation = USairports[ , c("local_code","name","latitude_deg","longitude_deg")]
# add missing value:
USairportLocation <- rbind(USairportLocation, c("PHL","Philadelphia International Airport", 39.874400, -75.242400))
USairportLocation <- rbind(USairportLocation, c("PHX","Phoenix Sky Harbor International Airport", 33.448400, -112.07400))
# link airport location
myABIA2 = merge(myABIA_fly,USairportLocation,by.x = "Dest", by.y = "local_code", all.x = TRUE)
colnames(myABIA2)[34:36] <- c("DestAirport", "D_lat", "D_long")
myABIA3 = merge(myABIA2,USairportLocation,by.x = "Origin", by.y = "local_code", all.x = TRUE)
colnames(myABIA3)[37:39] <- c("OriginAirport", "O_lat", "O_long")

myABIA3$D_lat <- as.numeric(myABIA3$D_lat)
myABIA3$O_lat <- as.numeric(myABIA3$O_lat)
myABIA3$D_long <- as.numeric(myABIA3$D_long)
myABIA3$O_long <- as.numeric(myABIA3$O_long)

# Mapping of different month
Air_summ <- myABIA3 %>%
  group_by(Month, INorOUT, Origin, Dest) %>%
  summarise(O_long = mean(O_long),D_long = mean(D_long),O_lat = mean(O_lat),D_lat = mean(D_lat),airline = length(INorOUT), Delay = mean(DepDelay))
Air_summ$airport[Air_summ$INorOUT == "Arrival"] <- Air_summ$Origin[Air_summ$INorOUT == "Arrival"]
Air_summ$airport[Air_summ$INorOUT == "Departure"] <- Air_summ$Dest[Air_summ$INorOUT == "Departure"]

Air_summ$OriginState <- map.where(database = "state", x = Air_summ$O_long, y = Air_summ$O_lat)
Air_summ$DestState <- map.where(database = "state", x = Air_summ$D_long, y = Air_summ$D_lat)
Air_summ$OriginState[Air_summ$Origin == "BOS"] <- "massachusetts"
Air_summ$DestState[Air_summ$Dest == "BOS"] <- "massachusetts"
Air_summ$OriginState[Air_summ$Origin == "OAK"] <- "california"
Air_summ$DestState[Air_summ$Dest == "OAK"] <- "california"
Air_summ$State[Air_summ$INorOUT == "Arrival"] <- Air_summ$OriginState[Air_summ$INorOUT == "Arrival"]
Air_summ$State[Air_summ$INorOUT == "Departure"] <- Air_summ$DestState[Air_summ$INorOUT == "Departure"]

Air_summ_state <- Air_summ %>%
  group_by(Month, INorOUT, State) %>%
  summarise(airline = sum(airline))

# https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf
# https://www2.census.gov/geo/docs/maps-data/maps/reg_div.txt
Regions <- data.frame(region = "New England", state = "connecticut", stringsAsFactors = FALSE)
Regions <- rbind(Regions, c("New England", "maine"))
Regions <- rbind(Regions, c("New England", "massachusetts"))
Regions <- rbind(Regions, c("New England", "new hampshire"))
Regions <- rbind(Regions, c("New England", "rhode island"))
Regions <- rbind(Regions, c("New England", "vermont"))

Regions <- rbind(Regions, c("Middle Atlantic", "new jersey"))
Regions <- rbind(Regions, c("Middle Atlantic", "new york"))
Regions <- rbind(Regions, c("Middle Atlantic", "new york:long island"))
Regions <- rbind(Regions, c("Middle Atlantic", "pennsylvania"))

Regions <- rbind(Regions, c("East North Central", "illinois"))
Regions <- rbind(Regions, c("East North Central", "indiana"))
Regions <- rbind(Regions, c("East North Central", "michigan"))
Regions <- rbind(Regions, c("East North Central", "michigan:south"))
Regions <- rbind(Regions, c("East North Central", "ohio"))
Regions <- rbind(Regions, c("East North Central", "wisconsin"))

Regions <- rbind(Regions, c("West North Central", "iowa"))
Regions <- rbind(Regions, c("West North Central", "kansas"))
Regions <- rbind(Regions, c("West North Central", "minnesota"))
Regions <- rbind(Regions, c("West North Central", "missouri"))
Regions <- rbind(Regions, c("West North Central", "nebraska"))
Regions <- rbind(Regions, c("West North Central", "north Dakota"))
Regions <- rbind(Regions, c("West North Central", "south Dakota"))

Regions <- rbind(Regions, c("South Atlantic", "Delaware"))
Regions <- rbind(Regions, c("South Atlantic", "district of columbia"))
Regions <- rbind(Regions, c("South Atlantic", "florida"))
Regions <- rbind(Regions, c("South Atlantic", "georgia"))
Regions <- rbind(Regions, c("South Atlantic", "maryland"))
Regions <- rbind(Regions, c("South Atlantic", "north carolina"))
Regions <- rbind(Regions, c("South Atlantic", "north carolina:main"))
Regions <- rbind(Regions, c("South Atlantic", "south carolina"))
Regions <- rbind(Regions, c("South Atlantic", "virginia"))
Regions <- rbind(Regions, c("South Atlantic", "virginia:main"))
Regions <- rbind(Regions, c("South Atlantic", "west virginia"))

Regions <- rbind(Regions, c("East South Central", "alabama"))
Regions <- rbind(Regions, c("East South Central", "kentucky"))
Regions <- rbind(Regions, c("East South Central", "mississippi"))
Regions <- rbind(Regions, c("East South Central", "tennessee"))

Regions <- rbind(Regions, c("West South Central", "arkansas"))
Regions <- rbind(Regions, c("West South Central", "louisiana"))
Regions <- rbind(Regions, c("West South Central", "oklahoma"))
Regions <- rbind(Regions, c("West South Central", "texas"))

Regions <- rbind(Regions, c("Mountain", "arizona"))
Regions <- rbind(Regions, c("Mountain", "colorado"))
Regions <- rbind(Regions, c("Mountain", "idaho"))
Regions <- rbind(Regions, c("Mountain", "montana"))
Regions <- rbind(Regions, c("Mountain", "nevada"))
Regions <- rbind(Regions, c("Mountain", "new mexico"))
Regions <- rbind(Regions, c("Mountain", "utah"))
Regions <- rbind(Regions, c("Mountain", "wyoming"))

Regions <- rbind(Regions, c("Pacific", "alaska"))
Regions <- rbind(Regions, c("Pacific", "california"))
Regions <- rbind(Regions, c("Pacific", "hawaii"))
Regions <- rbind(Regions, c("Pacific", "oregon"))
Regions <- rbind(Regions, c("Pacific", "washington"))
Regions <- rbind(Regions, c("Pacific", "washington:main"))

Air_summ_state = merge(Air_summ_state,Regions,by.x = "State", by.y = "state", all.x = TRUE)
Air_summ_state <- Air_summ_state %>%
  group_by(Month, INorOUT, region) %>%
  summarise(airline = sum(airline))
```


```{r Q1.2.7, echo = FALSE, warning=FALSE, fig.width=12, fig.height=8}

Air_summA = subset(Air_summ, Air_summ$INorOUT=="Arrival")
Air_summD = subset(Air_summ, Air_summ$INorOUT=="Departure")

# Mapping of all year
Air_summ2 <- myABIA3 %>%
  group_by(INorOUT, Origin, Dest) %>%
  summarise(O_long = mean(O_long),D_long = mean(D_long),O_lat = mean(O_lat),D_lat = mean(D_lat),airline = length(INorOUT), DepDelay = mean(DepDelay),ArrDelay = mean(ArrDelay))
Air_summ2$airport[Air_summ2$INorOUT == "Arrival"] <- Air_summ2$Origin[Air_summ2$INorOUT == "Arrival"]
Air_summ2$airport[Air_summ2$INorOUT == "Departure"] <- Air_summ2$Dest[Air_summ2$INorOUT == "Departure"]
Air_summ2$Delay[Air_summ2$INorOUT == "Departure"] <- Air_summ2$DepDelay[Air_summ2$INorOUT == "Arrival"]
Air_summ2$Delay[Air_summ2$INorOUT == "Arrival"] <- Air_summ2$ArrDelay[Air_summ2$INorOUT == "Departure"]
# num of flight
p9 = ggplot(data = Air_summ2)+
  geom_point(aes(O_long,O_lat))+
  geom_point(aes(D_long,D_lat))+
  geom_segment(aes(x = O_long, y = O_lat, xend = D_long, yend = D_lat, col = airline),size = 0.8)+
  facet_wrap(~ INorOUT)+
  labs(title = "Flight Volume Heat Map", x = " ", y = " ")+
  theme_gray()+
  theme(plot.title = element_text(hjust = 0.5), plot.margin = unit(c(1,1,1,1), "cm"),
        panel.grid.major = element_blank(),  panel.grid.minor = element_blank())+
  scale_x_discrete()+
  scale_y_discrete()+
  scale_colour_gradient2(low="white", high="Purple")+
  borders("state")+
  scale_size_area()+
  coord_quickmap()
p9
```

Overall, the top 5 airports with the most flights to Austin in 2008 are Dallas Love Field, Dallas/Fort Worth International, George Bush Intercontinental, Phoenix Sky Harbor International and Denver International airport with 5,464, 5,344, 3,651, 2,778 and 2,708 flights respectively. The top 5 destinations from Austin are exactly the same 5 in the same order with 5,442, 5,347, 3,636, 2,765 and 2,653 flights respectively. This result is represented in the heat map above.

```{r Q1.2.8, echo = FALSE, warning=FALSE}
p7 = ggplot(data = Air_summA)+
  geom_point(aes(O_long,O_lat))+
  geom_point(aes(D_long,D_lat))+
  geom_segment(aes(x = O_long, y = O_lat, xend = D_long, yend = D_lat, col = airline),size = 0.8)+
  theme_gray()+
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.major = element_blank(),  panel.grid.minor = element_blank())+
  scale_x_discrete()+
  scale_y_discrete()+
  scale_colour_gradient2(low="green", high="Purple")+
  borders("state")+
  scale_size_area()+
  coord_quickmap()+
  transition_states(
    Month,
    transition_length = 0,
    state_length = 1
    ) + 
  labs(title = 'Arrival Flight Volume Heat Map by Month: {closest_state}', x = " ", y = " ") +
  ease_aes('sine-in-out')
p7
```

```{r Q1.2.9, echo = FALSE, warning=FALSE}
p8 = ggplot(data = Air_summD)+
  geom_point(aes(O_long,O_lat))+
  geom_point(aes(D_long,D_lat))+
  geom_segment(aes(x = O_long, y = O_lat, xend = D_long, yend = D_lat, col = airline),size = 0.8)+
  theme_gray()+
  theme(plot.title = element_text(hjust = 0.5), 
        panel.grid.major = element_blank(),  panel.grid.minor = element_blank())+
  scale_x_discrete()+
  scale_y_discrete()+
  scale_colour_gradient2(low="green", high="Purple")+
  borders("state")+
  scale_size_area()+
  coord_quickmap()+
  transition_states(
    Month,
    transition_length = 0,
    state_length = 1
  ) + 
  labs(title = 'Departure Flight Volume Heat Map by Month: {closest_state}', x = ' ', y = ' ') +
  ease_aes('sine-in-out')
p8
```

In terms of the patterns of flights to Austin over the course of the year, it is clear from the heat map above that flight volume increases at the beginning of the year and over the summer. This is partially due to more airports with direct flights to Austin during this period. For example, Birmingham-Shuttlesworth International Airport and McGhee Tyson Airport only fly to Austin at the beginning of the year. And In the summer, Fort Lauderdale-Hollywood, Seattle-Tacoma International, and Indianapolis International are adding new services to Austin. Flights from Austin share the same pattern, with more flights at the beginning of year and in the summertime. 

```{r Q1.2.7_1, echo = FALSE, warning=FALSE}
P7_1 = ggplot(data = Air_summ_state, aes(x = Month, y = airline, col = region))+
  geom_point()+
  geom_line()+
  facet_wrap(~INorOUT, nrow = 2)+
  labs(title = "Month vs Number of Flight", x = "Month", y = "Number of Flight")+
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12),labels = c("1" = "Jan", "2" = "Feb", "3" = "Mar", "4" = "Apr", "5" = "May", "6" = "Jun", "7" = "Jul", "8" = "Aug", "9" = "Sep", "10" = "Oct", "11" = "Nov", "12" = "Dec"))+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
P7_1

Air_summA = subset(Air_summ, Air_summ$INorOUT=="Arrival")
Air_summD = subset(Air_summ, Air_summ$INorOUT=="Departure")
```

From the flight volume by region graph listed above, it can be seen that flights from West South Central contributed significantly to the high volume at the beginning of the year, and volume pops up in the summertime for Mountain, South Atlantic, Pacific, and East North Central regions. For detailed information about how the regions are divided, please visit: [Census Bureau Regions and Divisions with State FIPS Codes](https://www2.census.gov/geo/docs/maps-data/maps/reg_div.txt) 


### What are the bad airports to fly to?

```{r Q1.2.10, echo = FALSE, warning=FALSE}
# Q4: 
# delay of flight

Air_delay_departure = subset(Air_summ2, (Air_summ2$INorOUT == "Departure"))
p10 = ggplot(data = Air_delay_departure)+
  geom_bar(aes(x = reorder(airport, Delay), y = Delay),stat='identity',position='dodge')+
  coord_flip()+
  labs(title = "Average Departure Delay for Different Airport", y = "Departure Delay (min)", x = "Airport")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
p10
```

First, we considered the bad airports to fly to in terms of departure delay. We calculated average departure delay time for each airport during the year and plotted the bar chart to demonstrate average departure delay for different airports. From the graph, we can clearly see that Tucson international airport (TUS) had the longest departure delay time, almost 70 minutes. Nashville international airport (BNA), Harlingen Valley international airport (HRL), Phoenix Sky Harbor International Airport (PHX), Chicago O’Hare international airport (ORD) ranked 2nd, 3rd, 4th and 5th among the most departure delay airports, which departure delay time is approximately 38 minutes, 28 minutes, 21 minutes and 20 minutes respectively. Therefore, if we want to fly from Austin to the five cities aforementioned, especially Tucson, we need to have some preparation for departure delay, like watching a movie to kill off boring waiting time.

```{r Q1.2.11, echo = FALSE, warning=FALSE}
Air_delay_arrival = subset(Air_summ2, (Air_summ2$INorOUT == "Arrival"))
p11 = ggplot(data = Air_delay_arrival)+
  geom_bar(aes(x = reorder(airport, Delay), y = Delay),stat='identity',position='dodge')+
  coord_flip()+
  labs(title = "Average Arrival Delay for Different Airport", y = "Arrival Delay (min)", x = "Airport")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
p11
```

Second, we got the bad airports to fly to in terms of arrival delay. We calculated average arrival time for each airport and plotted the bar chart for average arrival delay of different airports. From the graph, We can clearly see that top 5 airports with most arrival delay are Dallas/Fort Worth International Airport (DFW),  Louis Armstrong New Orleans International Airport(MSY), Hartsfield-Jackson Atlanta International Airport (ATL), Fort Lauderdale-Hollywood International Airport (FLL) and El Paso International Airport (ELP). The airport at Dallas has the longest arrival delay time, which is almost 100 minutes. As a result, if we want to fly to these five cities from Austin, especially Dallas, we should take arrival delay time into consideration, by leaving enough time especially for transfer flights.

```{r Q1.2.12, echo = FALSE, warning=FALSE}
# delay of flight: mapping
p12 = ggplot(data = Air_summ2)+
  geom_point(aes(O_long,O_lat))+
  geom_point(aes(D_long,D_lat))+
  geom_segment(aes(x = O_long, y = O_lat, xend = D_long, yend = D_lat, col = Delay),size = 0.8)+
  facet_wrap(~ INorOUT)+
  scale_colour_gradient2(low="white", high="red")+
  borders("state")+
  labs(title = "Flight Delay Heat Map", x = " ", y = " ")+
  scale_size_area()+
  theme(plot.title = element_text(hjust = 0.5),
        panel.grid.major = element_blank(),  panel.grid.minor = element_blank())+
  scale_x_discrete()+
  scale_y_discrete()+
  coord_quickmap()
p12
```

Finally, we drew Austin Flight Delay Heat Map to show average arrival delay and departure delay of different airports around U.S.. On the map, the deeper the line color, the more delay time. According to arrival delay, the worst airport to fly to from Austin is Dallas/Fort Worth International Airport, which is consistent with our conclusion before. Regarding to departure delay, Arizona is the worst state to fly to, since its two biggest cities - Tucson and Phoenix have the worst delay time. 

## Exercise 1.3
```{r Ex3setup, include=FALSE}
urlfile<-'https://raw.githubusercontent.com/znzhao/ECO395M-HW-zz5339/master/HW1/sclass.csv'
sclass<-read.csv(url(urlfile))
summary(sclass)
sclass_350 <- subset(sclass,(sclass$trim == "350"))
sclass_65AMG <- subset(sclass,(sclass$trim == "65 AMG"))
# define a helper function for calculating RMSE
rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
```

In this exercise, we used K-nearest neighbors(KNN) to build a predictive model for price, given mileage, separately for each of two trim levels: 350 and 65 AMG.
First, we run the KNN models for the trim level of 350. Here is the plot for mileage vs price for the trim level of 350:

```{r s350, echo=FALSE, warning= FALSE, fig.align='center'}
# sclass_350
My_data = sclass_350

# plot the data
p_350 = ggplot(data = My_data) + 
  geom_point(mapping = aes(x = mileage, y = price), color='darkgrey')+
  theme_bw()+
  labs(title = "Mileage vs Price (Trim Level: 350)")+
  theme(plot.title = element_text(hjust = 0.5))
p_350
```

We splited the data into a training and a testing set. 80% of the data are in the training set. Then we run the  K-nearest-neighbors, for many different values of K, starting at K=3 and going as high as the sample size. For each value of K, we fit the model to the training set and make predictions on the test set, and calculate the out-of-sample root mean-squared error (RMSE) for each K. The plot of K versus RSME is shown below:

```{r s350p_KNNplot, echo=FALSE, warning=FALSE, fig.align='center'}
# Make a train-test split
N = nrow(My_data)
N_train = floor(0.8*N)
N_test = N - N_train

# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)

# Define the training and testing set
D_train = My_data[train_ind,]
D_test = My_data[-train_ind,]

# optional book-keeping step:
D_test = arrange(D_test, mileage)

# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)

KNN_result <- data.frame(K=c(), rsme=c())
# KNN
for(v in c(3:nrow(X_train))){
  # K = 2 generate some error?
  knn_K = knn.reg(train = X_train, test = X_test, y = y_train, k=v)
  ypred_knn = knn_K$pred
  KNN_rsme = rmse(y_test, ypred_knn)
  KNN_result <- rbind(KNN_result,c(v,KNN_rsme))
}

colnames(KNN_result) = c("K","RSME")
Kmin = KNN_result$K[which.min(KNN_result$RSME)]

P_KNNresult_350 = ggplot(data = KNN_result)+
  geom_line(aes(x = K, y = RSME))+
  geom_line(aes(x = Kmin, y = RSME), col = "red")+
  theme_bw()+
  labs(title = "K vs RSME(Trim Level: 350)")+
  theme(plot.title = element_text(hjust = 0.5))
P_KNNresult_350
```

The output of the optimal K for trim level 350 is printed below:

```{r showK1, echo=FALSE}
print(Kmin)
```
Since the training set is chosen randomly, if we run the KNN for several times, we will get different result. But generally, the optimal K for trim level 350 is about 19.
Then for the optimal value of K in this run, a plot of the fitted model is shown below:

```{r s350p_optimalplot, echo=FALSE, warning=FALSE, fig.align='center'}
knn_K = knn.reg(train = X_train, test = X_test, y = y_train, k=Kmin)
ypred_knn = knn_K$pred

D_test$ypred_knn = ypred_knn

p_test_350 = ggplot(data = D_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  geom_path(mapping = aes(x = mileage, y = ypred_knn), color='red') +
  theme_bw()+
  labs(title = "Fitted Model(Trim Level: 350)")+
  theme(plot.title = element_text(hjust = 0.5))
p_test_350
```

Second, we run the KNN models for the trim level of 65 AMG in a similar manner. Here is the plot for mileage vs price for the 65 AMG trim level:

```{r s65AMG, echo=FALSE, warning= FALSE, fig.align='center'}
# sclass_65AMG
My_data = sclass_65AMG


# plot the data
p_65 = ggplot(data = My_data) + 
  geom_point(mapping = aes(x = mileage, y = price), color='darkgrey')+
  theme_bw()+
  labs(title = "Mileage vs Price (Trim Level: 65 AMG)")+
  theme(plot.title = element_text(hjust = 0.5))
p_65
```

We splited the data into a training and a testing set. 80% of the data are in the training set. Then we run the  K-nearest-neighbors, for many different values of K, starting at K=3 and going as high as the sample size. For each value of K, fit the model to the training set and make predictions on the test set, and calculate the out-of-sample root mean-squared error (RMSE) for each value of K. The plot of K versus RSME is showed below:

```{r s65AMGp_KNNplot, echo=FALSE, warning=FALSE, fig.align='center'}
# Make a train-test split
N = nrow(My_data)
N_train = floor(0.8*N)
N_test = N - N_train

# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)

# Define the training and testing set
D_train = My_data[train_ind,]
D_test = My_data[-train_ind,]

# optional book-keeping step:
D_test = arrange(D_test, mileage)

# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)

# linear and quadratic models
KNN_result <- data.frame(K=c(), rsme=c())
# KNN
for(v in c(3:nrow(X_train))){
  # K = 2 generate some error?
  knn_K = knn.reg(train = X_train, test = X_test, y = y_train, k=v)
  ypred_knn = knn_K$pred
  KNN_rsme = rmse(y_test, ypred_knn)
  KNN_result <- rbind(KNN_result,c(v,KNN_rsme))
}

colnames(KNN_result) = c("K","RSME")
Kmin = KNN_result$K[which.min(KNN_result$RSME)]

p_test_65AMG = ggplot(data = KNN_result)+
  geom_line(aes(x = K, y = RSME))+
  geom_line(aes(x = Kmin, y = RSME), col = "red")+
  theme_bw()+
  labs(title = "K vs RSME(Trim Level: 65 AMG)")+
  theme(plot.title = element_text(hjust = 0.5))
p_test_65AMG
```

The output of the optimal K for trim level 65 AMG is printed below:

```{r showK2, echo=FALSE}
print(Kmin)
```

Since the training set is chosen randomly, if we run the KNN for several times, we will get different result. But generally, the optimal K for trim level 65 AMG is about 16.
Then for the optimal value of K in this run, a plot of the fitted model is shown below:

```{r s65p_optimalplot, echo=FALSE, warning=FALSE, fig.align='center'}
knn_K = knn.reg(train = X_train, test = X_test, y = y_train, k=Kmin)
ypred_knn = knn_K$pred

# attach the predictions to the test data frame

D_test$ypred_knn = ypred_knn

p_test_65 = ggplot(data = D_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  geom_path(mapping = aes(x = mileage, y = ypred_knn), color='red') +
  theme_bw()+
  labs(title = "Fitted Model(Trim Level: 65AMG)")+
  theme(plot.title = element_text(hjust = 0.5))
p_test_65 
```

In the end, we show that the average optimal K for trim level 350 is 19, which is larger than optimal K = 16 for trim level 65 AMG. We think that this is because the sample size for the first data subset is larger than the second, and hence has a larger optimal K.




