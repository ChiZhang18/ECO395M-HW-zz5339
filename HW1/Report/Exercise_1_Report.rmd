---
title: "Exercise 1"
author: "Chong Wang, Tinaping Wu, Zhenning Zhao"
date: "2019/2/9"
output:
  md_document:
    variant: markdown_github
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(RColorBrewer)
library(FNN)
```

# Exercise 1
By Chong Wang, Tinaping Wu, Zhenning Zhao

## Exercise 1.1
The environmentally friendly buildings have some obvious advantages, not only from an eco-friendly view, but also from the financial aspect. The "data guru" did not make a reasonable argument to quantity the advantages clearly. To be more specific, we believe that the duration that he/she calculated to recover the cost may be mistaken.

```{r p1.1, echo=FALSE, warning=FALSE, fig.align='center'}
urlfile<-'https://raw.githubusercontent.com/znzhao/ECO395M-HW-zz5339/master/HW1/greenbuildings.csv'
greenbuildings<-read.csv(url(urlfile))
greenbuildings$Green[greenbuildings$green_rating==0] <- "Non-Green"
greenbuildings$Green[greenbuildings$green_rating==1] <- "Green"
# Occupancy Rates & Rent Plot: NonGreen vs Green
NG <- subset(greenbuildings, (greenbuildings$green_rating == "0"))
p1 <- ggplot(data = greenbuildings) + 
  geom_point(mapping = aes(y = Rent, x = leasing_rate),alpha = 0.3, col = brewer.pal(6, "Greens")[5])+
  facet_wrap(~ Green)+
  geom_point(data = NG, mapping = aes(y = Rent, x = leasing_rate), alpha = 0.3, col = brewer.pal(6, "Reds")[5])+
  labs(title = "Occupancy Rates vs Rent Plot", 
       y = "Rent",
       x = "Occupancy Rates")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
p1
```

The first step is to clean the data. The "data guru" noticed that a handful of the buildings in the data set had very low occupancy rates. Although this is a true fact, which can be shown in the graph above, he/she did not provide a satisfactory reason for why the buildings with less than 10% leasing rate should be removed from consideration. Due to this fact, we disagree to simply delete these 215 data points. Instead, we were able to identify a reason to delete buildings with 0% occupancy rate. According to a [research conducted by IBM](https://www.ibm.com/support/knowledgecenter/en/SSFCZ3_10.5.2/com.ibm.tri.doc/wpm_metrics/r_occupancy_rate.html), buildings with a 0% leasing rate is called “not-used buildings”. Since our new building will certainly be used, we can comfortably exclude these data points from the original data set.

After that, we tried to clean the data even furthur. Since this new project is in Austin with the tallest building at 56 floors, clusters with buildings higher than this value are clearly not in Austin and therefore should be removed from the data set. As the result, we are left with 6,618 data points.

```{r p1.2, echo=FALSE, warning=FALSE, fig.align='center'}
# clean data by deleting the data with occupacy rate equal to 0% and taller than 56 floor
GB_test <- subset(greenbuildings,(greenbuildings$stories > 56))
Clus = unique(GB_test$cluster)

GB_cleaned <- subset(greenbuildings,(greenbuildings$leasing_rate != 0))
for (i in 1:39){
  GB_cleaned <-subset(GB_cleaned,(GB_cleaned$cluster!= Clus[i]))
}

# basic boxplot
NG_one <- subset(GB_cleaned, green_rating == "0")
p2 = ggplot(data = GB_cleaned, aes(x = Green, y = Rent)) + 
  geom_boxplot(fill = brewer.pal(6, "Greens")[5], alpha = 0.8)+
  geom_boxplot(data = NG_one, fill = brewer.pal(6, "Reds")[5], alpha= 0.9)+
  theme_bw()+
  labs(title = "Green Rating vs Rent Plot",
       x = "Green Rating",
       y = "Rent")+
  theme(plot.title = element_text(hjust = 0.5))
p2
```

According to the "data guru", there are still many outliers after data cleaning, so he/she chooses to use the median instead of the mean to calculate the expected rent to compare green and non-green buildings. We plot the boxplot of green and non-green buildings to get a overview of the data. It seems that the "data guru" was right about the outliers, but it is unclear why there are so many outliers. So we digged deeper, and got some interesting findings.

## Exercise 1.2

## Exercise 1.3
```{r Ex3setup, include=FALSE}
urlfile<-'https://raw.githubusercontent.com/znzhao/ECO395M-HW-zz5339/master/HW1/sclass.csv'
sclass<-read.csv(url(urlfile))
summary(sclass)
sclass_350 <- subset(sclass,(sclass$trim == "350"))
sclass_65AMG <- subset(sclass,(sclass$trim == "65 AMG"))
# define a helper function for calculating RMSE
rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
```

In this exercise, we used K-nearest neighbors(KNN) to build a predictive model for price, given mileage, separately for each of two trim levels: 350 and 65 AMG.
First, we run the KNN models for the trim level of 350. Here is the plot for mileage vs price for the trim level of 350:

```{r s350, echo=FALSE, warning= FALSE, fig.align='center'}
# sclass_350
My_data = sclass_350

# plot the data
p_350 = ggplot(data = My_data) + 
  geom_point(mapping = aes(x = mileage, y = price), color='darkgrey')+
  theme_bw()+
  labs(title = "Mileage vs Price (Trim Level: 350)")+
  theme(plot.title = element_text(hjust = 0.5))
p_350
```

We splited the data into a training and a testing set. There are 80% of the data are in the training set. Then we run the  K-nearest-neighbors, for many different values of K, starting at K=3 and going as high as the sample size. For each value of K, fit the model to the training set and make predictions on the test set, and calculate the out-of-sample root mean-squared error (RMSE) for each value of K. The plot of K versus RSME is showed below:

```{r s350p_KNNplot, echo=FALSE, warning=FALSE, fig.align='center'}
# Make a train-test split
N = nrow(My_data)
N_train = floor(0.8*N)
N_test = N - N_train

# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)

# Define the training and testing set
D_train = My_data[train_ind,]
D_test = My_data[-train_ind,]

# optional book-keeping step:
D_test = arrange(D_test, mileage)

# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)

KNN_result <- data.frame(K=c(), rsme=c())
# KNN
for(v in c(3:nrow(X_train))){
  # K = 2 generate some error?
  knn_K = knn.reg(train = X_train, test = X_test, y = y_train, k=v)
  ypred_knn = knn_K$pred
  KNN_rsme = rmse(y_test, ypred_knn)
  KNN_result <- rbind(KNN_result,c(v,KNN_rsme))
}

colnames(KNN_result) = c("K","RSME")
Kmin = KNN_result$K[which.min(KNN_result$RSME)]

P_KNNresult_350 = ggplot(data = KNN_result)+
  geom_line(aes(x = K, y = RSME))+
  geom_line(aes(x = Kmin, y = RSME), col = "red")+
  theme_bw()+
  labs(title = "K vs RSME(Trim Level: 350)")+
  theme(plot.title = element_text(hjust = 0.5))
P_KNNresult_350
```

The optimal K for trim level 350 is printed below:

```{r showK1, echo=FALSE}
print(Kmin)
```
Since the traning set is chosen randomly, if we run the KNN for several times, we will get different result. But generally, we can get the optimal K for trim level 350 is about 19.
Then for the optimal value of K in this one running, a plot of the fitted modelis shown below:

```{r s350p_optimalplot, echo=FALSE, warning=FALSE, fig.align='center'}
knn_K = knn.reg(train = X_train, test = X_test, y = y_train, k=Kmin)
ypred_knn = knn_K$pred

D_test$ypred_knn = ypred_knn

p_test_350 = ggplot(data = D_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  geom_path(mapping = aes(x = mileage, y = ypred_knn), color='red') +
  theme_bw()+
  labs(title = "Fitted Model(Trim Level: 350)")+
  theme(plot.title = element_text(hjust = 0.5))
p_test_350
```

Second, we run the KNN models for the trim level of 65 AMG similarly. Here is the plot for mileage vs price for the trim level of 65 AMG:

```{r s65AMG, echo=FALSE, warning= FALSE, fig.align='center'}
# sclass_65AMG
My_data = sclass_65AMG


# plot the data
p_65 = ggplot(data = My_data) + 
  geom_point(mapping = aes(x = mileage, y = price), color='darkgrey')+
  theme_bw()+
  labs(title = "Mileage vs Price (Trim Level: 65 AMG)")+
  theme(plot.title = element_text(hjust = 0.5))
p_65
```

We splited the data into a training and a testing set. There are 80% of the data are in the training set. Then we run the  K-nearest-neighbors, for many different values of K, starting at K=3 and going as high as the sample size. For each value of K, fit the model to the training set and make predictions on the test set, and calculate the out-of-sample root mean-squared error (RMSE) for each value of K. The plot of K versus RSME is showed below:

```{r s65AMGp_KNNplot, echo=FALSE, warning=FALSE, fig.align='center'}
# Make a train-test split
N = nrow(My_data)
N_train = floor(0.8*N)
N_test = N - N_train

# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)

# Define the training and testing set
D_train = My_data[train_ind,]
D_test = My_data[-train_ind,]

# optional book-keeping step:
D_test = arrange(D_test, mileage)

# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)

# linear and quadratic models
KNN_result <- data.frame(K=c(), rsme=c())
# KNN
for(v in c(3:nrow(X_train))){
  # K = 2 generate some error?
  knn_K = knn.reg(train = X_train, test = X_test, y = y_train, k=v)
  ypred_knn = knn_K$pred
  KNN_rsme = rmse(y_test, ypred_knn)
  KNN_result <- rbind(KNN_result,c(v,KNN_rsme))
}

colnames(KNN_result) = c("K","RSME")
Kmin = KNN_result$K[which.min(KNN_result$RSME)]

p_test_65AMG = ggplot(data = KNN_result)+
  geom_line(aes(x = K, y = RSME))+
  geom_line(aes(x = Kmin, y = RSME), col = "red")+
  theme_bw()+
  labs(title = "K vs RSME(Trim Level: 65 AMG)")+
  theme(plot.title = element_text(hjust = 0.5))
p_test_65AMG
```

The optimal K for trim level 65 AMG is printed below:

```{r showK2, echo=FALSE}
print(Kmin)
```

Since the traning set is chosen randomly, if we run the KNN for several times, we will get different result. But generally, we can get the optimal K for trim level 65 AMG is about 16.
Then for the optimal value of K in this one running, a plot of the fitted modelis shown below:

```{r s65p_optimalplot, echo=FALSE, warning=FALSE, fig.align='center'}
knn_K = knn.reg(train = X_train, test = X_test, y = y_train, k=Kmin)
ypred_knn = knn_K$pred

# attach the predictions to the test data frame

D_test$ypred_knn = ypred_knn

p_test_65 = ggplot(data = D_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  geom_path(mapping = aes(x = mileage, y = ypred_knn), color='red') +
  theme_bw()+
  labs(title = "Fitted Model(Trim Level: 65AMG)")+
  theme(plot.title = element_text(hjust = 0.5))
p_test_65 
```

In the end, it is showed that the average optimal K for trim level 350 is 19, which is larger than optimal K = 16 for trim level 65 AMG. We think that this is because that the sample size for the first data subset is larger than the second, and hence has a larger optimal K.




