---
title: "Exercise 1"
author: "Chong Wang, Tinaping Wu, Zhenning Zhao"
date: "2019/2/9"
output:
  md_document:
    variant: markdown_github
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(RColorBrewer)
library(FNN)
```
## Exercise 1.1

## Exercise 1.2

## Exercise 1.3
```{r Ex3setup, include=FALSE}
urlfile<-'https://raw.githubusercontent.com/znzhao/ECO395M-HW-zz5339/master/HW1/sclass.csv'
sclass<-read.csv(url(urlfile))
summary(sclass)
sclass_350 <- subset(sclass,(sclass$trim == "350"))
sclass_65AMG <- subset(sclass,(sclass$trim == "65 AMG"))
# define a helper function for calculating RMSE
rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
```

In this exercise, we used K-nearest neighbors(KNN) to build a predictive model for price, given mileage, separately for each of two trim levels: 350 and 65 AMG.
First, we run the KNN models for the trim level of 350. Here is the plot for mileage vs price for the trim level of 350:

```{r s350, echo=FALSE, warning= FALSE, fig.align='center'}
# sclass_350
My_data = sclass_350

# plot the data
p_350 = ggplot(data = My_data) + 
  geom_point(mapping = aes(x = mileage, y = price), color='darkgrey')+
  labs(title = "Mileage vs Price (Trim Level: 350)")+
  theme(plot.title = element_text(hjust = 0.5))
p_350
```

We splited the data into a training and a testing set. There are 80% of the data are in the training set. Then we run the  K-nearest-neighbors, for many different values of K, starting at K=3 and going as high as the sample size. For each value of K, fit the model to the training set and make predictions on the test set, and calculate the out-of-sample root mean-squared error (RMSE) for each value of K. The plot of K versus RSME is showed below:

```{r s350p_KNNplot, echo=FALSE, warning=FALSE, fig.align='center'}
# Make a train-test split
N = nrow(My_data)
N_train = floor(0.8*N)
N_test = N - N_train

#####
# Train/test split
#####

# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)

# Define the training and testing set
D_train = My_data[train_ind,]
D_test = My_data[-train_ind,]

# optional book-keeping step:
D_test = arrange(D_test, mileage)

# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)


#####
# Fit a few models
#####

KNN_result <- data.frame(K=c(), rsme=c())
# KNN
for(v in c(3:nrow(X_train))){
  # K = 2 generate some error?
  knn_K = knn.reg(train = X_train, test = X_test, y = y_train, k=v)
  ypred_knn = knn_K$pred
  KNN_rsme = rmse(y_test, ypred_knn)
  KNN_result <- rbind(KNN_result,c(v,KNN_rsme))
}

colnames(KNN_result) = c("K","RSME")
Kmin = KNN_result$K[which.min(KNN_result$RSME)]

P_KNNresult_350 = ggplot(data = KNN_result)+
  geom_line(aes(x = K, y = RSME))+
  geom_line(aes(x = Kmin, y = RSME), col = "red")+
  labs(title = "K vs RSME(Trim Level: 350)")+
  theme(plot.title = element_text(hjust = 0.5))
P_KNNresult_350
```