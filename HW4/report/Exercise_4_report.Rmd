---
title: "Exercise 4"
author: "Chong Wang, Tianping Wu, Zhenning Zhao"
date: "2019/4/26"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(LICORS)
library(ISLR)
library(mosaic)
library(foreach)
library(cluster)
library(corrplot)
library(plotly)
library(GGally)
library(ggplot2)
```

# Exercise 4
By Chong Wang, Tianping Wu, Zhenning Zhao

## Exercise 4.1 Clustering and PCA
```{r setup_4.1, echo=FALSE, warning=FALSE}
myurl <- "https://raw.githubusercontent.com/znzhao/ECO395M-HW-zz5339/master/HW4/wine.csv"
wine <- read.csv(url(myurl))
```

### Distinguishing the color of the wine

First we normalize the data. After demeaning and scaling with their standard deviation, we end up with a 6,497*11 dataset. The following is the heatmap of the correlation between these 11 chemical properties.

Although there are 11 chemical properties, we choose to visualize the data through only 4 dimensions: total sulfur dioxide, density, pH, and volatile acidity. The following graph shows the distribution of the red wines and the white wine on these 4 dimensions. We randomly pick these 4 properties to give a first taste of the data. From the graph we can tell that the red wine and the white wine have different features, so it is highly possible for us to distinguish these two different type of wines.

```{r plot4.1.1, echo=FALSE, warning=FALSE}
# Center and scale the data, data visualization
X = wine[,1:11]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
```

```{r plot4.1.2, echo=FALSE, warning=FALSE}
# distribution plot
XX = subset(wine,select = c("total.sulfur.dioxide","density","pH","volatile.acidity"))
ggpairs(XX,aes(col = wine$color, alpha = 0.8))
```

Since we have already have a basic impression of 2 categories in mind, we choose to do clustering with K=2.

First, by using K-means, we can divide the wines into 2 category. Visualizing through the total sulfur dioxide and the density, we can tell that K=means did an excellent work distinguishing red wines and white wines.

```{r plot4.1.3, echo=FALSE, warning=FALSE}
# First do clusting
clust1 = kmeans(X, 2, nstart=20)
qplot(wine$density,wine$total.sulfur.dioxide, data=wine, shape=factor(clust1$cluster), col=factor(wine$color))
res <- cor(X)
```

More specifically, we can calculate the accuracy rate by looking at the following confusion matrix. The accuracy rate for K-means is (4,830+1,575)/6,497 = 98.6%, which is pretty high. This means by looking at the chemical properties, the K-means can characterize the red wine and white wine almost perfectly.

```{r table4.1.4, echo=FALSE, warning=FALSE}
# table for the correctly clustering
xtabs(~clust1$cluster + wine$color)
table1 = xtabs(~clust1$cluster + wine$color)
```

Second, we use the PCA method. The summary of the scores is listed below. The first four principal components capture about 73% of the variance in the data. So I choose to use the first four principal components to do the clustering. The following is the graph of different wines and different categories on the scale of the first two components. As the graph shows, the PCA is also a good way to differ red wines from white wines.
```{r table4.1.5, echo=FALSE, warning=FALSE}
# Next try PCA
pc = prcomp(X, scale=TRUE)
summary(pc)
loadings = pc$rotation
scores = pc$x
# PCA for clustering
clustPCA = kmeans(scores[,1:4], 2, nstart=20)
qplot(scores[,1], scores[,2], color=factor(wine$color), shape=factor(clustPCA$cluster), xlab='Component 1', ylab='Component 2')
```

More specifically, we can calculate the accuracy rate by looking at the following confusion matrix. The accuracy rate for K-PCA is (4,818+1,575)/6,497 = 98.4%, which is slightly lower than the K-mean result. In conclusion, to differ white wines and red wines, we can simply use the K-mean method and it will give us a pretty good result.

```{r table4.1.6, echo=FALSE, warning=FALSE}
# table for the correctly clustering
xtabs(~clustPCA$cluster + wine$color)
tablePCA = xtabs(~clustPCA$cluster + wine$color)
```

### Distinguishing the quality of the wine

Before we do the clustering, the following barplot shows the distribution of the different qualities. There are only 7 different qualities of wines in the dataset. It seems that most of the wines have quality of 5 or 6, and only a few of them have very high or very low quality. Since normally the clustering method would divide the data into different categories quite equally, it might be very hard for K-means algorithm to successfully identify the quality of the wines.

```{r graph4.1.7, echo=FALSE, warning=FALSE}
# by the barplot we can see that most wines' quality is 6
ggplot(wine)+
  geom_bar(aes(x = quality))
``` 

What’s more, by data visualization, it seems that the wines with different qualities have similar chemistry features, making it even more difficult to identify the quality of the wine.

```{r graph4.1.8, echo=FALSE, warning=FALSE}
# it seems very hard to cluster them into 7 categories
ggpairs(XX,aes(col = factor(wine$quality),alpha = 0.6))
```

First, by using K-means, we can divide the wines into 7 category. The perfect density graph should be as follow. 

```{r graph4.1.9, echo=FALSE, warning=FALSE}
#the perfect density plot
ggplot(wine)+ geom_density(aes(x = wine$quality, col = factor(wine$quality), fill = factor(wine$quality)), alpha = 0.3)
```

However, the density of different wine should be concentrating on different categories. The result, as is shown in the following density graph and the confusion matrix, is not so good. There is no obvious pattern that could be pointed out from the clustering. Hence the K-mean method fails at this challenge, just as we expected.

```{r graph4.1.10, echo=FALSE, warning=FALSE}
# First do clusting
clust2 = kmeans(X, 7, nstart=20)

# table for the correctly clustering
xtabs(~clust2$cluster + wine$quality)
table2 = xtabs(~clust2$cluster + wine$quality)
# look what we got here! it looks very different from the perfect graph.
ggplot(wine)+ geom_density(aes(x = clust2$cluster, col = factor(wine$quality), fill = factor(wine$quality)), alpha = 0.3)
```

Second, we use the PCA method. Still we choose to use the first four principal components to do the clustering with K=7. The following is the graph of different wines qualities and different categories on the scale of the first two components. From the graph we can hardly tell any relations between the quality of the wine and the categories that we find.

```{r graph4.1.11, echo=FALSE, warning=FALSE}
# Next try PCA
pc = prcomp(X, scale=TRUE)
loadings = pc$rotation
scores = pc$x

# PCA for clustering
clustPCA2 = kmeans(scores[,1:4], 7, nstart=20)
qplot(scores[,1], scores[,2], color=factor(wine$quality), shape = factor(clustPCA2$cluster) , xlab='Component 1', ylab='Component 2')
```

The similar story can be told by looking at the confusion matrix and the density graph. However, the PCA method is slightly better than the K-means, since the high quality wine tends to cluster into similar categories. Saying that, the overall result of the prediction is still a nightmare. The chemistry feature just might not be the reason for the different qualities of the wine.

```{r graph4.1.12, echo=FALSE, warning=FALSE}
# table for the correctly clustering
xtabs(~clustPCA2$cluster + wine$quality)
tablePCA = xtabs(~clustPCA2$cluster + wine$quality)
ggplot(wine)+ geom_density(aes(x = clustPCA2$cluster, col = factor(wine$quality), fill = factor(wine$quality)), alpha = 0.3)
```

In conclusion, we might not be able to tell the difference among the different quality wine by only looking at the chemical features of the wine.



## Exercise 4.2 Market segmentation
### 4.2.1 Data pre-process
First we decided to eliminate as many bots as possible from the slip through. All users with spam posts are assumed to be pots as only a few dozens of them had spam posts. Users with pornography posts are a bit complicated because more than a few couple hundred users had them and at the same time also posted significant amount of other types of posts, so they might just be actual human users with interests in pornography to some extent . To distinguish between humans and bots, we set an arbitrary rule of 20/80 to delete all users having more than 20% of their total posts in pornagraphy. Next, column chatter and uncategorized  are deleted because they are the labels that do not fit at all into any of the interest categories. At the end, we are left with 7,676 users to determine market segmentation using clustering and principal components analysis methodologies. At last, there are 33 variables left.

```{r setup_4.2.1, echo=FALSE, warning=FALSE}
urlfile<-'https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv'

#36 different categories
SocialMarket <- read.csv(url(urlfile), row.names=1)
# head(SocialMarket, 10)

#delete users with spam
SocialMarket<-SocialMarket[(SocialMarket$spam==0),]

#delete uncategorized label "chatter"
SocialMarket <- subset(SocialMarket, select = -c(chatter, uncategorized))

#add tweet sum & calculate adult ratio & delete adult ratio more than 20%
SocialMarket <- cbind(tweet_sum = rowSums(SocialMarket), SocialMarket)
SocialMarket <- cbind(adult_ratio = 1, SocialMarket)
SocialMarket$adult_ratio <- SocialMarket$adult/SocialMarket$tweet_sum
SocialMarket<-SocialMarket[(SocialMarket$adult_ratio<0.2),]

#delete uncategorized label "unused attributes"
SocialMarket <- subset(SocialMarket, select = -c(adult_ratio, tweet_sum, spam))

# Center/scale the data
#SocialMarket = SocialMarket[,-(1,35)]
SocialMarket_scaled <- scale(SocialMarket, center=TRUE, scale=TRUE) 
N = nrow(SocialMarket)
```

### 4.2.2 Clustering
In order to determine market segment by k-means clustering, we must first select the number of initial centroids, or in other words, the number of user types. 3 types of supporting analysis were used to help us determine the quantity: Elbow plot(SSE), CH index and Gap statistics.

```{r graph_4.2.1, echo=FALSE, warning=FALSE}
#K-grid to find the optimal K
k_grid = seq(2, 20, by=1)
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(SocialMarket_scaled, k, nstart=50)
  cluster_k$tot.withinss
}
graphics.off()
#par("mar")
par(mar=c(4,4,4,4))

plot(k_grid, SSE_grid, xlab="K",ylab="SSE Grid", sub="SSE Grid vs K")
```

```{r graph_4.2.1.1, echo=FALSE, warning=FALSE}
#CH-grid to find the optimal K
CH_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(SocialMarket_scaled, k, nstart=20)
  W = cluster_k$tot.withinss
  B = cluster_k$betweenss
  CH = (B/W)*((N-k)/(k-1))
  CH
}

plot(k_grid, CH_grid, xlab="K",
     ylab="CH Grid",
     sub="CH Grid vs K")
```

```{r graph_4.2.1.2, echo=FALSE, warning=FALSE}
#Gap statistics
Market_gap = clusGap(SocialMarket_scaled, FUN = kmeans, nstart = 20, K.max = 10, B = 10)
plot(Market_gap)
```

As shown above, the results are subtle and therefore difficult to determine the best number for K. We eventually picked K=7 for two reasons, 1. we observed a weak signal of dipping in the Gap statistic graph and 2. we found about the equal number of interest groups with relatively strong correlated interests from our correlation analysis as shown below.

```{r graph_4.2.2, echo=FALSE, warning=FALSE}
#correlation and visualization
res <- cor(SocialMarket_scaled)
corrplot(res, method = "color", tl.cex = 0.5, tl.col="black")
```

We created this heat map hoping to have a deeper analysis of each cluster. Even though we would never know the full picture of each cluster, we believed interests with high proximity, or high correlation, would most likely be fit into same cluster. The more common interests we find from each cluster, the better we can describe each market segment and therefore are able to help our client creating cluster based market strategies.

```{r model_4.2.1, echo=FALSE, warning=FALSE}
# k-means analysis
clust1 = kmeans(SocialMarket_scaled, centers=7, nstart=25)
```

Some distinct market segments with highly correlated interests are listed below based on the heat map

#### 1. Personal fitness, outdoors, health & nutrition

```{r graph_4.2.3, echo=FALSE, warning=FALSE}
XX = subset(SocialMarket,select = c("personal_fitness","health_nutrition","outdoors"))
ggpairs(XX,aes(col = factor(clust1$cluster), alpha = 0.8))
```

#### 2. Fashion, cooking, beauty, shopping, photo sharing

```{r graph_4.2.4.1, echo=FALSE, warning=FALSE}
XX = subset(SocialMarket,select = c("fashion","cooking","beauty", "shopping", "photo_sharing"))
ggpairs(XX,aes(col = factor(clust1$cluster), alpha = 0.8))
```

#### 3. Online gaming, college&university, sports playing

```{r graph_4.2.5, echo=FALSE, warning=FALSE}
XX = subset(SocialMarket,select = c("online_gaming","college_uni","sports_playing"))
ggpairs(XX,aes(col = factor(clust1$cluster), alpha = 0.8))
```

#### 4. Sports fandom, food, family, religion, parenting, school

``````{r graph_4.2.6.1, echo=FALSE, warning=FALSE}
XX = subset(SocialMarket,select = c("sports_fandom","parenting","school","food", "family"))
ggpairs(XX,aes(col = factor(clust1$cluster), alpha = 0.8))
```

#### 5. Politics, news, computers, travel, automobiles

```{r graph_4.2.7.1, echo=FALSE, warning=FALSE}
XX = subset(SocialMarket,select = c("politics","news","computers", "travel", "automotive"))
ggpairs(XX,aes(col = factor(clust1$cluster), alpha = 0.8))
```

#### 6. TV film, art, music

```{r graph_4.2.8, echo=FALSE, warning=FALSE}
XX = subset(SocialMarket,select = c("tv_film","art","music"))
ggpairs(XX,aes(col = factor(clust1$cluster), alpha = 0.8))
```

#### 7. Everything, shopping, photo sharing 
From the graphs above, we can see the last group being a very special one, showing moderate interests in almost all areas (compared to strong distinct tastes in other groups).  Within the group, interests toward shopping and photo sharing seems to stand out.


### 4.2.3 Principal Components Analysis

After data pre-process, In order to reduce dimension of 33 different categories variables, we decided to use principal components analysis methods to find principal components, which can explain most of the variability in the data.

After center and scale the data, we did the correlation analysis of total 33 categories first. In the correlation matrix above, we found that the correlation of those categories are relatively weak, as most correlation coefficients are below 0.3. Thus, we suppose that the proportion of variance explained by most dominant principal components will not be as high as we expected.

We first got the loadings matrix and scores matrix from principal components methods. Then we calculated proportion of variance explained (PVE) to decide the number of principal components that we need to choose. 

```{r table_4.2.1, echo=FALSE, warning=FALSE}

urlfile<-'https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv'
social_marketing = read.csv(url(urlfile), row.names=1)

#delete users with spam
social_marketing = social_marketing[(social_marketing$spam==0),]

#delete uncategorized label "chatter"
social_marketing = subset(social_marketing, select = -c(chatter, uncategorized))

#add tweet sum & calculate adult ratio & delete adult ratio more than 20%
social_marketing = cbind(tweet_sum = rowSums(social_marketing), social_marketing)
social_marketing = cbind(adult_ratio = 1, social_marketing)
social_marketing$adult_ratio = social_marketing$adult/social_marketing$tweet_sum
social_marketing = social_marketing[(social_marketing$adult_ratio<0.2),]

#delete uncategorized label "unused attributes"
social_marketing = subset(social_marketing, select = -c(adult_ratio, tweet_sum, spam))

#center and scale the data
social_marketing = scale(social_marketing, center=TRUE, scale=TRUE)

# correlation
cor=cor(social_marketing)

# PCA
pca = prcomp(social_marketing,scale=TRUE)
loadings = pca$rotation
scores = pca$x

# PVE
VE = pca$sdev^2
PVE = VE / sum(VE)
round(PVE, 2)
```

In the above table, we can see that the first eight principal components can explain most of the variability. The first principal component explains 13% of the variability; the second principal component explains 8% of the variability; the third principal component explains 8% of the variability;the fourth principal component explains 7% of the variability; the fifth principal component explains 7% of the variability; the sixth principal component explains 5% of the variability; the seventh principal component explains 4% of the variability; the eighth principal component explains 4% of the variability. Together, the first eight principal components explain 56% of the variability.

```{r graph_4.2.9, echo=FALSE, warning=FALSE}
PVEplot = qplot(c(1:33), PVE) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("PVE") +
  ggtitle("Scree Plot") +
  ylim(0, 0.15)
PVEplot
```

```{r graph_4.2.10, echo=FALSE, warning=FALSE}
cumPVE = qplot(c(1:33), cumsum(PVE)) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab(NULL) +
  ggtitle("Cumulative Scree Plot") +
  ylim(0,1)
cumPVE
```

In the PVE Plot, we can see that between eighth and ninth components, there’s a significant gap in the Scree Plot. Also, from the Cumulative PVE Plot, we can find that first eight principal components can explain more than 50% of the total variability. Thus, we choose 8 principal components to divide the market of NutrientH20 into 8 segments. The characteristics of these 8 market segments are actually latent factor inferred from 33 interests categories.

Then we got top 5 interests of followers of NutrientH20 in each market segment.

```{r table_4.2.2, echo=FALSE, warning=FALSE}
# extract market segments
o1 = order(loadings[,1], decreasing=TRUE)
colnames(social_marketing)[head(o1,5)]

o2 = order(loadings[,2], decreasing=TRUE)
colnames(social_marketing)[head(o2,5)]

o3 = order(loadings[,3], decreasing=TRUE)
colnames(social_marketing)[head(o3,5)]

o4 = order(loadings[,4], decreasing=TRUE)
colnames(social_marketing)[head(o4,5)]

o5 = order(loadings[,5], decreasing=TRUE)
colnames(social_marketing)[head(o5,5)]

o6 = order(loadings[,6], decreasing=TRUE)
colnames(social_marketing)[head(o6,5)]

o7 = order(loadings[,7], decreasing=TRUE)
colnames(social_marketing)[head(o7,5)]

o8 = order(loadings[,8], decreasing=TRUE)
colnames(social_marketing)[head(o8,5)]
```

In the 1st market segment, top 5 interest of followers are "religion", "food", "parenting", "sports_fandom" and "school".

In the 2nd market segment, top 5 interest of followers are "sports_fandom", "religion", "parenting", "food" and "school".

In the 1st and 2nd market segment, the top 5 interests are same, so we combine them into one segment as new 1st market segment.

In the 2nd market segment, top 5 interest of followers are  "politics", "travel", "computers", "news" and "automotive".

In the 3rd market segment, top 5 interest of followers are  "health_nutrition", "personal_fitness", "outdoors", "politics" and "news".

In the 4th market segment, top 5 interest of followers are "beauty", "fashion", "cooking", "photo_sharing" and "shopping". 

In the 5th market segment, top 5 interest of followers are "online_gaming", "sports_playing", "college_uni", "cooking" and "automotive". 

In the 6th market segment, top 5 interest of followers are "automotive", "shopping", "photo_sharing", "news" and "current_events".

In the 7th market segment, top 5 interest of followers are "news", "automotive", "tv_film", "art" and "beauty". 

Finally, we extracted 7 market segments.

### 4.2.4 Conclusion

From the clustering and principal component analysis, we extracted 7 analysis from both of them. 
The first market segment found by clustering is similar with the third segment found by PCA as they have same interests - Personal fitness, outdoors and health & nutrition. 

The second market segment found by clustering is similar with the fourth segment found by PCA as they have same interests - Fashion, cooking, beauty, shopping and photo sharing. 

The third market segment found by clustering is similar with the fifth segment found by PCA as they have same interests - Online gaming, college&university and sports playing. 

The fourth market segment found by clustering is similar with the first segment found by PCA as they have same interests - Sports fandom, food, religion, parenting and school.

The fifth market segment found by clustering is similar with the second segment found by PCA as they have same interests - Politics, news, computers, travel and automobiles. 

The sixth market segment found by clustering is similar with the seventh segment found by PCA as they have similar interests - TV film and art. 

The seventh market segment found by clustering is similar with the sixth segment found by PCA as they have similar interests - shopping and photo sharing.

Finally, we labeled above seven market segments to show their unique characteristics.

We named the first market segment as “Mr. fitness”. Those kinds of people focus on working out and keeping in a good shape.

We named the second market segment as “Mrs. fashion”. Those kinds of people like keeping up with fashion and sharing their happy moments with friends.

We named the third market segment as “typical college student”. College students consist with most parts of this group. They are fond of entertainment such as online games and sports during their rest time.

We named the fourth market segment as “middle-age parents”. They care about the fostering of their children. Also, they have interests in sports games. 

We named the fifth market segment as “business man”. They pay attention to daily news online. Also, they like travelling during vacation. 

We named the sixth market segment as “Hippie”. They like visiting gallery and enjoying movies.

We named the seventh market segment as “Typical online user with interests toward everything but mainly shopping and photo sharing”. This is the typical you and me. 



## Exercise 4.3 Association rules for grocery purchases

For the given shopping baskets, first we draw the barplot of the 20 most popular goods among the consumers. From the graph below, the most popular good is whole milk, followed by other vegetables, rolls and buns, soda and yogurt.

Then we use the criteria that support >= 0.01, confidence >= 0.1, maxlen <= 5 to find all the association rules. The criteria is very ad hoc, but it did help us find 435 rules in total. The following table is the summary of all the association rules.

Because there are too many rules, we looked at the subsets of the rules. 

First, the following rules have lift>=3, indicating strong connections. There are 8 rules having lift>=3. For example, the first rule shows that if a consumer bought beef, the probability that she also bought root vegetables are three times higher than the prior probability of buying root vegetables. It is highly possible that this type of consumers is buying them to make a beef soup or roasted beef or other beef dishes that includes root vegetables.

Second, there are 14 rules having confidence > 0.5. For example, the first rule shows that if a consumer bought curd and yogurt, the probability of buying whole milk is 0.58. It’s highly possible that this type of consumers wants to mix curd, yogurt and whole to make desserts.
Third, as the thresholds we set above are too strict that once we combine them there would be too little associations, we purposely relaxed the constraints. After contemplating the significance of the associations and the overall visualization, we set lift>2 & confidence>0.3, resulting 44 rules. For example, the first rule shows that if a consumer bought onions, the probability that he also bought other vegetables are 2.3 times higher than the prior probability of buying other vegetables. With the same rule, if he bought onions, the probability of buying other vegetables is 0.46. It’s highly possible that this type of consumers wants to use onions and other vegetables to make salad. 

